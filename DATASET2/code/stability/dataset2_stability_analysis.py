import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import glob
from scipy.stats import entropy
from pathlib import Path
import warnings
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['Microsoft YaHei']  # æŒ‡å®šä¸­æ–‡å­—ä½“
matplotlib.rcParams['axes.unicode_minus'] = False  # æ­£å¸¸æ˜¾ç¤ºè´Ÿå·
warnings.filterwarnings('ignore')

# -------- å‚æ•°è®¾ç½® --------
AREA_COL = "area"             # é¢ç§¯åˆ—
BINS = 50                     # binæ•°é‡
RANGE = (500, 3500)           # é¢ç§¯åˆ†å¸ƒèŒƒå›´
STEP = 50                    # æ¯æ¬¡é€’å¢æ ·æœ¬æ•°
MAX_SAMPLE = 50000            # æœ€å¤§é‡‡æ ·æ•°é‡
THRESHOLD = 0.001           # ç›¸ä¼¼åº¦å˜åŒ–é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
MIN_SIMILARITY = 0.95       # ç›¸ä¼¼åº¦æœ€ä½è¦æ±‚ï¼ˆæ›´ä¸¥æ ¼ï¼‰
CONSECUTIVE = 5               # è¿ç»­å‡ æ¬¡ Î” < é˜ˆå€¼ åˆ¤å®šç¨³å®š

# æ¨èç­–ç•¥é€‰æ‹©: "75th" (75åˆ†ä½æ•°), "90th" (90åˆ†ä½æ•°), "max" (æœ€å¤§å€¼)
RECOMMENDATION_STRATEGY = "75th"  # å¯ä¿®æ”¹ä¸º "90th" æˆ– "max"

# -------- ç›¸ä¼¼åº¦å‡½æ•°ï¼šHistogram Intersection --------
def compute_histogram_intersection(hist1, hist2):
    hist1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    hist2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    return np.sum(np.minimum(hist1, hist2)) / np.sum(np.maximum(hist1, hist2))

# -------- KLæ•£åº¦å‡½æ•° --------
def compute_kl_divergence(p, q):
    p = np.asarray(p, dtype=np.float64) + 1e-10
    q = np.asarray(q, dtype=np.float64) + 1e-10
    return np.sum(p * np.log(p / q))

# -------- åˆ†æå•ä¸ªæ–‡ä»¶çš„ç¨³å®šæ€§ --------
def analyze_single_file(csv_path, area_col=AREA_COL):
    """åˆ†æå•ä¸ªCSVæ–‡ä»¶çš„ç¨³å®šæ€§"""
    try:
        df = pd.read_csv(csv_path)
        if area_col not in df.columns:
            print(f"âš ï¸ {csv_path}: æœªæ‰¾åˆ°åˆ— '{area_col}'")
            return None
            
        area_data = df[area_col].dropna().to_numpy()
        if len(area_data) < STEP * 2:
            print(f"âš ï¸ {csv_path}: æ•°æ®é‡ä¸è¶³ ({len(area_data)} < {STEP * 2})")
            return None
            
        area_data = np.sort(area_data)  # æ’åºç¡®ä¿ç¨³å®šé‡‡æ ·
        total_n = min(len(area_data), MAX_SAMPLE)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = [1.0]
        prev_hist = None
        bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
        
        for n in range(STEP, total_n + STEP, STEP):
            current_sample = area_data[:n]
            hist, _ = np.histogram(current_sample, bins=bins, density=True)
            
            if prev_hist is not None:
                sim = compute_histogram_intersection(hist, prev_hist)
                kl = compute_kl_divergence(hist, prev_hist)
                similarities.append(sim)
                delta = sim - similarities[-2]
                print(f"Samples: {n}, Intersection Similarity: {sim:.4f}, Î” = {delta:+.4f}, KLæ•£åº¦: {kl:.4f}")
            prev_hist = hist
        
        # åˆ¤æ–­ç¨³å®šç‚¹
        deltas = np.abs(np.diff(similarities))
        stable_index = -1
        for i in range(len(deltas) - CONSECUTIVE + 1):
            # æ—¢è¦å˜åŒ–å°ï¼Œåˆè¦ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜
            if (np.all(deltas[i:i+CONSECUTIVE] < THRESHOLD) and 
                similarities[i+CONSECUTIVE] >= MIN_SIMILARITY):
                stable_index = (i + 1) * STEP
                print(f"   âœ“ åœ¨æ ·æœ¬æ•° {stable_index} å¤„è¾¾åˆ°ç¨³å®š (ç›¸ä¼¼åº¦: {similarities[i+CONSECUTIVE]:.4f})")
                break
        
        return {
            'file_path': csv_path,
            'total_cells': len(area_data),
            'stable_sample_size': stable_index,
            'similarities': similarities[1:],
            'sample_sizes': list(range(STEP * 2, STEP * (len(similarities) + 1), STEP)),
            'area_data': area_data  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºåç»­åˆ†æ
        }
        
    except Exception as e:
        print(f"âŒ {csv_path}: å¤„ç†å¤±è´¥ - {str(e)}")
        return None

# -------- æŸ¥æ‰¾æ‰€æœ‰merged.csvæ–‡ä»¶ --------
def find_all_merged_files(base_dir="."):
    """æŸ¥æ‰¾æ‰€æœ‰merged.csvæ–‡ä»¶"""
    pattern = os.path.join(base_dir, "**", "total", "merged.csv")
    files = glob.glob(pattern, recursive=True)
    return sorted(files)

# -------- åœ¨ç‰¹å®šæ ·æœ¬æ•°ä¸‹çš„ç›¸ä¼¼åº¦æ’å€¼å‡½æ•° --------
def get_similarity_at_sample_size(result, target_sample_size):
    """ä½¿ç”¨çº¿æ€§æ’å€¼ä¼°ç®—åœ¨ç‰¹å®šæ ·æœ¬æ•°ä¸‹çš„ç›¸ä¼¼åº¦"""
    sample_sizes = result['sample_sizes']
    similarities = result['similarities']
    
    if target_sample_size <= sample_sizes[0]:
        return similarities[0]
    if target_sample_size >= sample_sizes[-1]:
        return similarities[-1]
    
    # çº¿æ€§æ’å€¼
    return np.interp(target_sample_size, sample_sizes, similarities)

# -------- ä¸»åˆ†æå‡½æ•° --------
def analyze_dataset2_stability():
    """åˆ†ææ•´ä¸ªDATASET2çš„ç¨³å®šæ€§"""
    print("ğŸ” å¼€å§‹åˆ†æDATASET2çš„ç»†èƒé¢ç§¯åˆ†å¸ƒç¨³å®šæ€§...")
    
    # æŸ¥æ‰¾æ‰€æœ‰merged.csvæ–‡ä»¶
    merged_files = find_all_merged_files()
    print(f"ğŸ“ æ‰¾åˆ° {len(merged_files)} ä¸ªmerged.csvæ–‡ä»¶")
    
    if not merged_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•merged.csvæ–‡ä»¶")
        return
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    results = []
    for file_path in merged_files:
        print(f"ğŸ“Š åˆ†æ: {file_path}")
        result = analyze_single_file(file_path)
        if result:
            results.append(result)
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•æ–‡ä»¶")
        return None
    
    # æ±‡æ€»ç»“æœ
    print(f"\nğŸ“ˆ æˆåŠŸåˆ†æ {len(results)} ä¸ªæ–‡ä»¶")
    
    # æå–ç¨³å®šæ ·æœ¬æ•°é‡
    stable_sizes = [r['stable_sample_size'] for r in results if r['stable_sample_size'] != -1]
    total_cells = [r['total_cells'] for r in results]
    
    # è®¡ç®—æ¨èæ ·æœ¬æ•°ï¼ˆç”¨äºåç»­åˆ†ç±»ï¼‰
    if stable_sizes:
        recommended_size_for_classification = int(np.percentile(stable_sizes, 75))
    else:
        recommended_size_for_classification = int(np.percentile(total_cells, 50))
    
    # åˆ†ç±»ç»Ÿè®¡
    stable_files = [r for r in results if r['stable_sample_size'] != -1]
    unstable_files = [r for r in results if r['stable_sample_size'] == -1]
    
    # è¿›ä¸€æ­¥åˆ†ç±»ä¸ç¨³å®šæ–‡ä»¶ï¼ˆä½¿ç”¨DATASET2è‡ªå·±çš„æ¨èå€¼ï¼‰
    insufficient_samples = [r for r in unstable_files if r['total_cells'] < recommended_size_for_classification]
    poor_quality = [r for r in unstable_files if any(s != s for s in r['similarities'][:3])]  # æœ‰nanå€¼
    slow_converging = [r for r in unstable_files if r not in insufficient_samples and r not in poor_quality]
    
    print(f"   ğŸ“Š ç¨³å®šæ–‡ä»¶: {len(stable_files)} ä¸ª")
    print(f"   âš ï¸  ä¸ç¨³å®šæ–‡ä»¶: {len(unstable_files)} ä¸ª")
    print(f"      - æ ·æœ¬é‡ä¸è¶³: {len(insufficient_samples)} ä¸ª (< {recommended_size_for_classification})")
    print(f"      - æ•°æ®è´¨é‡é—®é¢˜: {len(poor_quality)} ä¸ª") 
    print(f"      - ç¼“æ…¢æ”¶æ•›: {len(slow_converging)} ä¸ª")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ–‡ä»¶æ•°: {len(results)}")
    print(f"   - æ‰¾åˆ°ç¨³å®šç‚¹çš„æ–‡ä»¶æ•°: {len(stable_sizes)}")
    print(f"   - å¹³å‡ç»†èƒæ€»æ•°: {np.mean(total_cells):.0f}")
    print(f"   - ä¸­ä½æ•°ç»†èƒæ€»æ•°: {np.median(total_cells):.0f}")
    
    if stable_sizes:
        print(f"   - ç¨³å®šæ ·æœ¬æ•°èŒƒå›´: {min(stable_sizes)} - {max(stable_sizes)}")
        print(f"   - ç¨³å®šæ ·æœ¬æ•°ä¸­ä½æ•°: {np.median(stable_sizes):.0f}")
        print(f"   - ç¨³å®šæ ·æœ¬æ•°å¹³å‡å€¼: {np.mean(stable_sizes):.0f}")
        
        # ä¸åŒç­–ç•¥çš„æ¨èæ ·æœ¬æ•°é‡ï¼ˆç¡®ä¿æ˜¯STEPçš„å€æ•°ï¼‰
        percentile_75_raw = np.percentile(stable_sizes, 75)
        percentile_90_raw = np.percentile(stable_sizes, 90)
        
        # å‘ä¸Šå–æ•´åˆ°æœ€è¿‘çš„STEPå€æ•°
        percentile_75 = int(np.ceil(percentile_75_raw / STEP) * STEP)
        percentile_90 = int(np.ceil(percentile_90_raw / STEP) * STEP)
        max_size = max(stable_sizes)  # æœ€å¤§å€¼æœ¬èº«å°±æ˜¯STEPçš„å€æ•°
        
        print(f"\nğŸ¯ ä¸åŒç­–ç•¥çš„æ¨èæ ·æœ¬æ•°é‡:")
        print(f"   ğŸ“Š 75åˆ†ä½æ•°ç­–ç•¥: {percentile_75} (åŸå§‹å€¼: {percentile_75_raw:.1f}, è¦†ç›–75%æ–‡ä»¶)")
        print(f"   ğŸ“Š 90åˆ†ä½æ•°ç­–ç•¥: {percentile_90} (åŸå§‹å€¼: {percentile_90_raw:.1f}, è¦†ç›–90%æ–‡ä»¶)")
        print(f"   ğŸ“Š æœ€å¤§å€¼ç­–ç•¥: {max_size} (è¦†ç›–100%æ–‡ä»¶)")
        
        # æˆæœ¬æ•ˆç›Šåˆ†æ
        print(f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:")
        print(f"   - 75åˆ†ä½æ•° â†’ 90åˆ†ä½æ•°: å¢åŠ {percentile_90-percentile_75}æ ·æœ¬ (+{(percentile_90-percentile_75)/percentile_75*100:.1f}%), å¤šè¦†ç›–{90-75}%æ–‡ä»¶")
        print(f"   - 90åˆ†ä½æ•° â†’ æœ€å¤§å€¼: å¢åŠ {max_size-percentile_90}æ ·æœ¬ (+{(max_size-percentile_90)/percentile_90*100:.1f}%), å¤šè¦†ç›–{100-90}%æ–‡ä»¶")
        print(f"   - 75åˆ†ä½æ•° â†’ æœ€å¤§å€¼: å¢åŠ {max_size-percentile_75}æ ·æœ¬ (+{(max_size-percentile_75)/percentile_75*100:.1f}%), å¤šè¦†ç›–{100-75}%æ–‡ä»¶")
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©æ¨èæ ·æœ¬æ•°
        if RECOMMENDATION_STRATEGY == "90th":
            recommended_size = percentile_90
        elif RECOMMENDATION_STRATEGY == "max":
            recommended_size = max_size
        else:  # é»˜è®¤ä½¿ç”¨75åˆ†ä½æ•°
            recommended_size = percentile_75
        
        print(f"\nğŸ¯ å½“å‰é‡‡ç”¨ç­–ç•¥: {RECOMMENDATION_STRATEGY} = {recommended_size}")
        if RECOMMENDATION_STRATEGY == "75th":
            print(f"   (å¹³è¡¡æˆæœ¬ä¸è¦†ç›–ç‡ï¼Œå¦‚éœ€100%è¦†ç›–å¯é€‰æ‹©{max_size})")
        elif RECOMMENDATION_STRATEGY == "90th":
            print(f"   (é«˜è¦†ç›–ç‡ç­–ç•¥ï¼Œå¦‚éœ€100%è¦†ç›–å¯é€‰æ‹©{max_size})")
        else:
            print(f"   (100%è¦†ç›–ç­–ç•¥ï¼Œä¿è¯æ‰€æœ‰æ–‡ä»¶éƒ½ç¨³å®š)")
        
        # åˆ†æè¶…å‡ºæ¨èå€¼çš„æ–‡ä»¶
        stable_files = [r for r in results if r['stable_sample_size'] != -1]
        above_recommended = [r for r in stable_files if r['stable_sample_size'] > recommended_size]
        if above_recommended:
            print(f"\nâš ï¸  è¶…å‡ºå½“å‰æ¨èæ ·æœ¬æ•°çš„æ–‡ä»¶ ({len(above_recommended)}ä¸ªï¼Œå {len(above_recommended)/len(stable_files)*100:.1f}%):")
            for r in above_recommended:
                file_name = os.path.basename(os.path.dirname(os.path.dirname(r['file_path'])))
                similarity_at_recommended = get_similarity_at_sample_size(r, recommended_size)
                print(f"   ğŸ“ {file_name}: éœ€è¦{r['stable_sample_size']}æ ·æœ¬ç¨³å®š, åœ¨{recommended_size}æ ·æœ¬æ—¶ç›¸ä¼¼åº¦={similarity_at_recommended:.4f}")
            
            if RECOMMENDATION_STRATEGY == "75th":
                print(f"\nğŸ’¡ ç­–ç•¥å»ºè®®:")
                avg_similarity_at_75 = np.mean([get_similarity_at_sample_size(r, percentile_75) for r in above_recommended])
                print(f"   - è‹¥ä½¿ç”¨75åˆ†ä½æ•°({percentile_75}): æœªç¨³å®šæ–‡ä»¶çš„å¹³å‡ç›¸ä¼¼åº¦={avg_similarity_at_75:.4f}")
                if avg_similarity_at_75 >= 0.80:
                    print(f"   - âœ… å»ºè®®: å¯ä½¿ç”¨75åˆ†ä½æ•°ï¼Œé£é™©å¯æ§")
                else:
                    print(f"   - âš ï¸  å»ºè®®: è€ƒè™‘ä½¿ç”¨90åˆ†ä½æ•°({percentile_90})æˆ–æœ€å¤§å€¼({max_size})")
        else:
            print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æ¨èæ ·æœ¬æ•°{recommended_size}å†…è¾¾åˆ°ç¨³å®š")

    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¨³å®šç‚¹")
        median_cells = np.median(total_cells)
        # ç¡®ä¿æ¨èæ ·æœ¬æ•°æ˜¯STEPçš„å€æ•°
        recommended_size = int(np.ceil(median_cells / STEP) * STEP)
        print(f"\nğŸ¯ æ¨èçš„ä»£è¡¨æ€§æ ·æœ¬æ•°é‡: {recommended_size}")
        print(f"   (åŸºäºæ€»ç»†èƒæ•°ä¸­ä½æ•° {median_cells:.0f} å‘ä¸Šå–æ•´åˆ°{STEP}çš„å€æ•°)")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    save_detailed_results(results, recommended_size)
    
    # ç»˜åˆ¶æ±‡æ€»å›¾è¡¨
    plot_summary_results(results, stable_sizes, recommended_size)
    
    # å•ç‹¬ç»˜åˆ¶æ‰€æœ‰æ–‡ä»¶ç›¸ä¼¼åº¦æ›²çº¿
    plot_all_similarity_curves(results, recommended_size)
    
    return results, recommended_size

# -------- ä¿å­˜è¯¦ç»†ç»“æœ --------
def save_detailed_results(results, recommended_size):
    """ä¿å­˜è¯¦ç»†çš„åˆ†æç»“æœ"""
    # åˆ›å»ºç»“æœDataFrame
    data = []
    for r in results:
        # è®¡ç®—500-3500èŒƒå›´å†…çš„æœ‰æ•ˆç»†èƒæ•°
        if 'area_data' in r:
            effective_cells = len([x for x in r['area_data'] if RANGE[0] <= x <= RANGE[1]])
        else:
            effective_cells = None
            
        data.append({
            'file_path': r['file_path'],
            'total_cells': r['total_cells'],
            'effective_cells_500_3500': effective_cells,
            'stable_sample_size': r['stable_sample_size'] if r['stable_sample_size'] != -1 else None,
            'has_stable_point': r['stable_sample_size'] != -1
        })
    
    df_results = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°Excelï¼ˆä¿å­˜åˆ°å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(script_dir, "dataset2_stability_analysis.xlsx")
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        df_results.to_excel(writer, sheet_name='è¯¦ç»†ç»“æœ', index=False)
        
        # åˆ›å»ºæ±‡æ€»è¡¨
        summary_data = {
            'æŒ‡æ ‡': ['æ€»æ–‡ä»¶æ•°', 'æ‰¾åˆ°ç¨³å®šç‚¹æ–‡ä»¶æ•°', 'å¹³å‡ç»†èƒæ€»æ•°', 'ä¸­ä½æ•°ç»†èƒæ€»æ•°', 
                    'ç¨³å®šæ ·æœ¬æ•°æœ€å°å€¼', 'ç¨³å®šæ ·æœ¬æ•°æœ€å¤§å€¼', 'ç¨³å®šæ ·æœ¬æ•°ä¸­ä½æ•°', 'ç¨³å®šæ ·æœ¬æ•°å¹³å‡å€¼', 'æ¨èæ ·æœ¬æ•°'],
            'æ•°å€¼': [len(results), len([r for r in results if r['stable_sample_size'] != -1]),
                    np.mean([r['total_cells'] for r in results]),
                    np.median([r['total_cells'] for r in results]),
                    min([r['stable_sample_size'] for r in results if r['stable_sample_size'] != -1]) if any(r['stable_sample_size'] != -1 for r in results) else None,
                    max([r['stable_sample_size'] for r in results if r['stable_sample_size'] != -1]) if any(r['stable_sample_size'] != -1 for r in results) else None,
                    np.median([r['stable_sample_size'] for r in results if r['stable_sample_size'] != -1]) if any(r['stable_sample_size'] != -1 for r in results) else None,
                    np.mean([r['stable_sample_size'] for r in results if r['stable_sample_size'] != -1]) if any(r['stable_sample_size'] != -1 for r in results) else None,
                    recommended_size]
        }
        df_summary = pd.DataFrame(summary_data)
        df_summary.to_excel(writer, sheet_name='æ±‡æ€»ç»Ÿè®¡', index=False)
    
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

# -------- ç»˜åˆ¶æ±‡æ€»å›¾è¡¨ --------
def plot_summary_results(results, stable_sizes, recommended_size):
    """ç»˜åˆ¶æ±‡æ€»ç»“æœå›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. ç¨³å®šæ ·æœ¬æ•°åˆ†å¸ƒ
    if stable_sizes:
        axes[0, 0].hist(stable_sizes, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].axvline(recommended_size, color='red', linestyle='--', linewidth=2, label=f'æ¨èå€¼: {recommended_size}')
        axes[0, 0].set_title('ç¨³å®šæ ·æœ¬æ•°åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('ç¨³å®šæ ·æœ¬æ•°')
        axes[0, 0].set_ylabel('æ–‡ä»¶æ•°é‡')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
    
    # 2. æ€»ç»†èƒæ•°åˆ†å¸ƒ
    total_cells = [r['total_cells'] for r in results]
    axes[0, 1].hist(total_cells, bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].axvline(np.median(total_cells), color='red', linestyle='--', linewidth=2, 
                      label=f'ä¸­ä½æ•°: {np.median(total_cells):.0f}')
    axes[0, 1].set_title('æ€»ç»†èƒæ•°åˆ†å¸ƒ')
    axes[0, 1].set_xlabel('æ€»ç»†èƒæ•°')
    axes[0, 1].set_ylabel('æ–‡ä»¶æ•°é‡')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. ç¨³å®šæ ·æœ¬æ•° vs æ€»ç»†èƒæ•°æ•£ç‚¹å›¾
    if stable_sizes:
        stable_files = [r for r in results if r['stable_sample_size'] != -1]
        stable_cells = [r['total_cells'] for r in stable_files]
        stable_samples = [r['stable_sample_size'] for r in stable_files]
        
        axes[1, 0].scatter(stable_cells, stable_samples, alpha=0.6, color='orange')
        axes[1, 0].axhline(recommended_size, color='red', linestyle='--', linewidth=2, 
                          label=f'æ¨èå€¼: {recommended_size}')
        axes[1, 0].set_title('ç¨³å®šæ ·æœ¬æ•° vs æ€»ç»†èƒæ•°')
        axes[1, 0].set_xlabel('æ€»ç»†èƒæ•°')
        axes[1, 0].set_ylabel('ç¨³å®šæ ·æœ¬æ•°')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # 4. æ‰€æœ‰æ–‡ä»¶çš„ç›¸ä¼¼åº¦æ›²çº¿
    if len(results) > 0:
        # ç»˜åˆ¶æ‰€æœ‰æ–‡ä»¶çš„ç›¸ä¼¼åº¦æ›²çº¿
        for i, result in enumerate(results):
            if len(result['similarities']) > 0:
                # ä½¿ç”¨ä¸åŒçš„é¢œè‰²å’Œé€æ˜åº¦
                color = plt.cm.get_cmap('viridis')(i / len(results))
                alpha = 0.6 if i < 10 else 0.3  # å‰10ä¸ªæ–‡ä»¶æ›´æ˜æ˜¾ï¼Œåé¢çš„æ›´é€æ˜
                
                axes[1, 1].plot(result['sample_sizes'], result['similarities'], 
                               marker='o', markersize=2, alpha=alpha, 
                               color=color, linewidth=1)
        
        # æ·»åŠ æ¨èæ ·æœ¬æ•°çš„å‚ç›´çº¿
        axes[1, 1].axvline(x=recommended_size, color='red', linestyle='--', linewidth=2, 
                          label=f'æ¨èæ ·æœ¬æ•°: {recommended_size}')
        axes[1, 1].axhline(y=1.0, color='gray', linestyle='--', linewidth=1)
        
        axes[1, 1].set_title(f'æ‰€æœ‰æ–‡ä»¶ç›¸ä¼¼åº¦æ›²çº¿ (å…±{len(results)}ä¸ªæ–‡ä»¶)')
        axes[1, 1].set_xlabel('æ ·æœ¬æ•°é‡')
        axes[1, 1].set_ylabel('ç›´æ–¹å›¾äº¤é›†ç›¸ä¼¼åº¦')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡åˆ°å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    image_file = os.path.join(script_dir, "dataset2_stability_summary.png")
    plt.savefig(image_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š æ±‡æ€»å›¾è¡¨å·²ä¿å­˜åˆ°: {image_file}")

# -------- å•ç‹¬ç»˜åˆ¶æ‰€æœ‰æ–‡ä»¶ç›¸ä¼¼åº¦æ›²çº¿ --------
def plot_all_similarity_curves(results, recommended_size):
    """å•ç‹¬ç»˜åˆ¶æ‰€æœ‰æ–‡ä»¶çš„ç›¸ä¼¼åº¦æ›²çº¿"""
    if len(results) == 0:
        print("âš ï¸ æ²¡æœ‰ç»“æœæ•°æ®ï¼Œæ— æ³•ç»˜åˆ¶ç›¸ä¼¼åº¦æ›²çº¿")
        return
    
    # åˆ›å»ºæ›´å¤§çš„å›¾å½¢
    plt.figure(figsize=(15, 10))
    
    # é¦–å…ˆç»˜åˆ¶æ‰€æœ‰æ–‡ä»¶ä½œä¸ºèƒŒæ™¯ï¼ˆä½é€æ˜åº¦ï¼‰
    for i, result in enumerate(results):
        if len(result['similarities']) > 0:
            plt.plot(result['sample_sizes'], result['similarities'], 
                    color='gray', alpha=0.15, linewidth=0.8)
    
    # é€‰æ‹©ä»£è¡¨æ€§æ–‡ä»¶è¿›è¡Œçªå‡ºæ˜¾ç¤º
    stable_results = [r for r in results if r['stable_sample_size'] != -1]
    if stable_results:
        # æŒ‰ç¨³å®šæ ·æœ¬æ•°æ’åº
        stable_results.sort(key=lambda x: x['stable_sample_size'])
        
        # é€‰æ‹©ä»£è¡¨æ€§æ–‡ä»¶ï¼šæœ€å°ã€25åˆ†ä½ã€ä¸­ä½æ•°ã€75åˆ†ä½ã€æœ€å¤§
        n = len(stable_results)
        if n >= 5:
            representative_indices = [0, n//4, n//2, 3*n//4, n-1]
            labels = ['æœ€å°ç¨³å®šæ ·æœ¬æ•°', '25åˆ†ä½æ•°', 'ä¸­ä½æ•°', '75åˆ†ä½æ•°', 'æœ€å¤§ç¨³å®šæ ·æœ¬æ•°']
            colors = ['blue', 'green', 'orange', 'purple', 'red']
        elif n >= 3:
            representative_indices = [0, n//2, n-1]
            labels = ['æœ€å°ç¨³å®šæ ·æœ¬æ•°', 'ä¸­ä½æ•°', 'æœ€å¤§ç¨³å®šæ ·æœ¬æ•°']
            colors = ['blue', 'orange', 'red']
        else:
            representative_indices = list(range(n))
            labels = [f'æ–‡ä»¶{i+1}' for i in range(n)]
            colors = ['blue', 'orange'][:n]
        
        # ç»˜åˆ¶ä»£è¡¨æ€§æ›²çº¿
        for idx, rep_idx in enumerate(representative_indices):
            result = stable_results[rep_idx]
            file_name = os.path.basename(os.path.dirname(os.path.dirname(result['file_path'])))
            
            plt.plot(result['sample_sizes'], result['similarities'], 
                    color=colors[idx], linewidth=3, alpha=0.8,
                    label=f'{labels[idx]} ({file_name})')
    
    # å¦‚æœæ²¡æœ‰ç¨³å®šçš„æ–‡ä»¶ï¼Œé€‰æ‹©å‰5ä¸ªæ–‡ä»¶
    else:
        for i in range(min(5, len(results))):
            result = results[i]
            if len(result['similarities']) > 0:
                file_name = os.path.basename(os.path.dirname(os.path.dirname(result['file_path'])))
                color = plt.cm.get_cmap('tab10')(i)
                
                plt.plot(result['sample_sizes'], result['similarities'], 
                        color=color, linewidth=3, alpha=0.8,
                        label=f'æ–‡ä»¶{i+1} ({file_name})')
    
    file_count = len([r for r in results if len(r['similarities']) > 0])
    
    # æ·»åŠ æ¨èæ ·æœ¬æ•°çš„å‚ç›´çº¿
    plt.axvline(x=recommended_size, color='red', linestyle='--', linewidth=3, 
                label=f'æ¨èæ ·æœ¬æ•°: {recommended_size}')
    
    # æ·»åŠ å®Œç¾ç›¸ä¼¼åº¦çš„æ°´å¹³çº¿
    plt.axhline(y=1.0, color='gray', linestyle='--', linewidth=2, alpha=0.7, 
                label='å®Œç¾ç›¸ä¼¼åº¦ (1.0)')
    
    # æ·»åŠ ç¨³å®šæ€§é˜ˆå€¼çº¿
    plt.axhline(y=MIN_SIMILARITY, color='orange', linestyle=':', linewidth=2, alpha=0.7, 
                label=f'æœ€ä½ç›¸ä¼¼åº¦è¦æ±‚ ({MIN_SIMILARITY})')
    
    # è®¾ç½®å›¾å½¢å±æ€§
    stable_count = len([r for r in results if r['stable_sample_size'] != -1])
    plt.title(f'DATASET2 ç›¸ä¼¼åº¦æ›²çº¿åˆ†æ\n(å…± {file_count} ä¸ªæ–‡ä»¶ï¼Œ{stable_count} ä¸ªæœ‰ç¨³å®šç‚¹)', fontsize=16, fontweight='bold')
    plt.xlabel('æ ·æœ¬æ•°é‡', fontsize=14)
    plt.ylabel('ç›´æ–¹å›¾äº¤é›†ç›¸ä¼¼åº¦', fontsize=14)
    
    # è®¾ç½®å›¾ä¾‹
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    
    # è®¾ç½®ç½‘æ ¼
    plt.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
    
    # è®¾ç½®åæ ‡è½´èŒƒå›´
    plt.ylim(0.7, 1.02)  # èšç„¦åœ¨é«˜ç›¸ä¼¼åº¦åŒºåŸŸ
    
    # æ·»åŠ æ–‡æœ¬è¯´æ˜
    plt.text(0.02, 0.98, f'å‚æ•°è®¾ç½®:\nâ€¢ Bins: {BINS}\nâ€¢ èŒƒå›´: {RANGE}\nâ€¢ é˜ˆå€¼: {THRESHOLD}\nâ€¢ æœ€ä½ç›¸ä¼¼åº¦: {MIN_SIMILARITY}', 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜å›¾ç‰‡åˆ°å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    image_file = os.path.join(script_dir, "dataset2_all_similarity_curves.png")
    plt.savefig(image_file, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"ğŸ“Š æ‰€æœ‰æ–‡ä»¶ç›¸ä¼¼åº¦æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {image_file}")
    
    # è¾“å‡ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ ç›¸ä¼¼åº¦æ›²çº¿ç»Ÿè®¡:")
    print(f"   - æ€»æ–‡ä»¶æ•°: {file_count}")
    print(f"   - æ‰€æœ‰æ–‡ä»¶ä»¥ç°è‰²èƒŒæ™¯æ›²çº¿æ˜¾ç¤º")
    
    stable_count = len([r for r in results if r['stable_sample_size'] != -1])
    if stable_count > 0:
        print(f"   - æœ‰ç¨³å®šç‚¹çš„æ–‡ä»¶æ•°: {stable_count}")
        print(f"   - ä»£è¡¨æ€§æ–‡ä»¶å·²ç”¨å½©è‰²çªå‡ºæ˜¾ç¤º")
    else:
        print(f"   - æ²¡æœ‰æ‰¾åˆ°ç¨³å®šç‚¹ï¼Œæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶ä½œä¸ºä»£è¡¨")
    
    # åˆ†æåœ¨æ¨èæ ·æœ¬æ•°å¤„çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
    similarities_at_recommended = []
    for result in results:
        if len(result['similarities']) > 0:
            sim_at_rec = get_similarity_at_sample_size(result, recommended_size)
            similarities_at_recommended.append(sim_at_rec)
    
    if similarities_at_recommended:
        avg_sim = np.mean(similarities_at_recommended)
        min_sim = np.min(similarities_at_recommended)
        max_sim = np.max(similarities_at_recommended)
        print(f"   - åœ¨æ¨èæ ·æœ¬æ•°{recommended_size}å¤„:")
        print(f"     â€¢ å¹³å‡ç›¸ä¼¼åº¦: {avg_sim:.4f}")
        print(f"     â€¢ æœ€ä½ç›¸ä¼¼åº¦: {min_sim:.4f}")
        print(f"     â€¢ æœ€é«˜ç›¸ä¼¼åº¦: {max_sim:.4f}")
        below_threshold = len([s for s in similarities_at_recommended if s < MIN_SIMILARITY])
        print(f"     â€¢ ä½äºæœ€ä½è¦æ±‚çš„æ–‡ä»¶: {below_threshold}ä¸ª ({below_threshold/len(similarities_at_recommended)*100:.1f}%)")

# -------- ä¸»ç¨‹åº --------
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹DATASET2ç¨³å®šæ€§åˆ†æ...")
    print("=" * 60)
    
    result_data = analyze_dataset2_stability()
    if result_data is not None:
        results, recommended_size = result_data
    else:
        results, recommended_size = [], 0
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ¯ æ¨èç”¨äºæ•´ä¸ªDATASET2çš„ä»£è¡¨æ€§æ ·æœ¬æ•°é‡: {recommended_size}")
    
    # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    excel_file = os.path.join(script_dir, "dataset2_stability_analysis.xlsx")
    summary_image_file = os.path.join(script_dir, "dataset2_stability_summary.png")
    curves_image_file = os.path.join(script_dir, "dataset2_all_similarity_curves.png")
    
    print("ğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   - {excel_file} (è¯¦ç»†ç»“æœ)")
    print(f"   - {summary_image_file} (æ±‡æ€»å›¾è¡¨)")
    print(f"   - {curves_image_file} (æ‰€æœ‰æ–‡ä»¶ç›¸ä¼¼åº¦æ›²çº¿)")

