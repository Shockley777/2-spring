import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.ndimage import gaussian_filter1d
from scipy.stats import wasserstein_distance, pearsonr, entropy
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
import shutil
import tempfile
from datetime import datetime


# åŸºæœ¬å‚æ•°
DATASET1_BASE = r"D:\project\2-spring\DATASET1\data"
AREA_COL = "area"
BINS = 50
RANGE = (500, 3500)
ALL_GROUPS_DAYS = [f"DAY{i}" for i in range(1, 7)]
ALL_GROUPS_DATAFOLDERS = [f"data{j}" for j in range(1, 6)]
SAMPLE_SIZES = [2500, 3000, 3500, 4000, 4500, 5000]  # å›ºå®šæŠ½æ ·ç»†èƒæ•°ï¼Œå¯æŒ‰éœ€æ‰©å±•
TOP_K = 10
RESULT_DIR = os.path.join("similarity", "results", "intra_dataset1")

# æ˜¯å¦ä½¿ç”¨ç¨³å®šæ€§åˆ†ææ¥å†³å®šæŠ½æ ·é‡ï¼›ä»¥åŠç¨³å®šæ€§æ–‡ä»¶è·¯å¾„ä¸ç­–ç•¥
USE_STABILITY = True
STABILITY_XLSX = r"D:\project\2-spring\DATASET1\code\stability\dataset1_stability_analysis.xlsx"
# æ¥æºæ¨¡å¼ï¼š'from_excel' | 'vs_full'
STABILITY_MODE = 'vs_full'
# ç­–ç•¥ï¼š'per_file' æ¯ä¸ªæ–‡ä»¶ç”¨è‡ªå·±çš„ç¨³å®šç‚¹ï¼›'global_75th' / 'global_90th' / 'global_max'
STABILITY_STRATEGY = 'per_file'

# vs_full ç¨³å®šæ€§å‚æ•°ï¼ˆä¸å…¨é‡æ›²çº¿æ¯”è¾ƒï¼‰
STAB_THRESHOLD = 0.98
STAB_STEP = 250
STAB_CONSECUTIVE = 3
STAB_REPEATS = 3  # æ¯ä¸ªæ ·æœ¬é‡é‡å¤æŠ½æ ·ä»¥é™ä½æ–¹å·®

# æŒ‡æ ‡è®¾ç½®
SIMILARITY_METHODS = [
    'intersection',    # è¶Šå¤§è¶Šå¥½
    'cosine',          # è¶Šå¤§è¶Šå¥½
    'pearson',         # è¶Šå¤§è¶Šå¥½
    'chi_square',      # è¶Šå°è¶Šå¥½
    'kl',              # è¶Šå°è¶Šå¥½
    'wasserstein',     # è¶Šå°è¶Šå¥½
]
DISPLAY_NAMES = {
    'intersection': 'Histogram Intersection',
    'cosine': 'Cosine Similarity',
    'pearson': 'Pearson Correlation',
    'chi_square': 'Chi-Square Distance',
    'kl': 'KL Divergence',
    'wasserstein': 'Wasserstein Distance',
}
HIGHER_BETTER = {'intersection', 'cosine', 'pearson'}
PLOT_METHOD = 'intersection'


def ensure_dir(path: str) -> None:
    if not os.path.exists(path):
        os.makedirs(path)


def get_writable_output_dir(preferred_dir: str, min_free_mb: int = 50) -> str:
    env_dir = os.environ.get("INTRA_DATASET1_OUTDIR")
    candidates = []
    if env_dir:
        candidates.append(env_dir)
    candidates.append(preferred_dir)
    candidates.append(os.path.join(tempfile.gettempdir(), "intra_dataset1"))

    for path in candidates:
        try:
            os.makedirs(path, exist_ok=True)
            total, used, free = shutil.disk_usage(path)
            if free >= min_free_mb * 1024 * 1024:
                return path
        except Exception:
            continue

    fallback = candidates[-1]
    try:
        os.makedirs(fallback, exist_ok=True)
    except Exception:
        pass
    print("âš ï¸ æ‰€æœ‰è¾“å‡ºç›®å½•ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå°†å°è¯•å†™å…¥ä¸´æ—¶ç›®å½•ï¼Œå¯èƒ½ä»ä¼šå¤±è´¥ã€‚")
    return fallback


def load_area_series(base_dir: str, day_folder: str, data_folder: str) -> np.ndarray | None:
    csv_path = os.path.join(base_dir, day_folder, data_folder, "total", "merged.csv")
    if not os.path.exists(csv_path):
        return None
    df = pd.read_csv(csv_path)
    if AREA_COL not in df.columns:
        return None
    area_series = df[AREA_COL].dropna().values
    area_series = area_series[(area_series >= RANGE[0]) & (area_series <= RANGE[1])]
    if area_series.size < 5:
        return None
    return area_series


def build_hist_from_sample(area_values: np.ndarray, sample_size: int) -> tuple[np.ndarray, np.ndarray] | tuple[None, None]:
    if area_values.size < sample_size:
        return None, None
    idx = np.random.choice(area_values.size, size=sample_size, replace=False)
    sampled = area_values[idx]
    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(sampled, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


def build_full_hist(area_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(area_values, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


def compute_similarity(hist1: np.ndarray, hist2: np.ndarray, method: str, bin_centers: np.ndarray) -> float:
    h1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    h2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    h1 /= np.sum(h1)
    h2 /= np.sum(h2)
    if method == 'intersection':
        return float(np.sum(np.minimum(h1, h2)))
    elif method == 'cosine':
        return float(cosine_similarity(h1.reshape(1, -1), h2.reshape(1, -1))[0, 0])
    elif method == 'pearson':
        corr, _ = pearsonr(h1, h2)
        return float(0 if np.isnan(corr) else corr)
    elif method == 'chi_square':
        denom = h1 + h2
        return float(0.5 * np.sum(((h1 - h2) ** 2) / denom))
    elif method == 'kl':
        return float(entropy(h1, h2))
    elif method == 'wasserstein':
        return float(wasserstein_distance(bin_centers, bin_centers, h1, h2))
    else:
        raise ValueError(f"Unsupported similarity method: {method}")


def load_stability_map(xlsx_path: str) -> tuple[dict[str, int], int] | tuple[dict[str, int], None]:
    """è¯»å–ç¨³å®šæ€§åˆ†æExcelï¼Œè¿”å›ï¼šæ¯ç»„çš„ç¨³å®šæ ·æœ¬æ•°æ˜ å°„ã€å…¨å±€75åˆ†ä½æ¨èå€¼ã€‚

    Excel ç”± `dataset1_stability_analysis.py` ç”Ÿæˆï¼Œsheet='è¯¦ç»†ç»“æœ'ï¼Œå«åˆ—ï¼š
    - file_path: .../DAYx/dataY/total/merged.csv
    - stable_sample_size: ç¨³å®šç‚¹ï¼ˆå¯èƒ½ä¸ºNaNï¼‰
    """
    if not os.path.exists(xlsx_path):
        print(f"âš ï¸ æœªæ‰¾åˆ°ç¨³å®šæ€§æ–‡ä»¶ï¼š{xlsx_path}ï¼Œå°†ä½¿ç”¨å›ºå®š SAMPLE_SIZESã€‚")
        return {}, None

    try:
        df = pd.read_excel(xlsx_path, sheet_name='è¯¦ç»†ç»“æœ')
    except Exception as e:
        print(f"âš ï¸ è¯»å–ç¨³å®šæ€§æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†ä½¿ç”¨å›ºå®š SAMPLE_SIZESã€‚")
        return {}, None

    def path_to_key(p: str) -> str | None:
        try:
            parts = os.path.normpath(p).split(os.sep)
            # æœŸæœ› .../DAYx/dataY/total/merged.csv
            if len(parts) >= 4:
                day = parts[-4]
                data = parts[-3]
                return f"{day}_{data}"
            return None
        except Exception:
            return None

    stability_map: dict[str, int] = {}
    stable_sizes: list[int] = []
    for _, row in df.iterrows():
        fp = str(row.get('file_path', ''))
        key = path_to_key(fp)
        val = row.get('stable_sample_size', None)
        if key and pd.notna(val) and int(val) > 0:
            stability_map[key] = int(val)
            stable_sizes.append(int(val))

    global_75th = int(np.percentile(stable_sizes, 75)) if stable_sizes else None
    return stability_map, global_75th


def compute_hist_iou(hist1: np.ndarray, hist2: np.ndarray) -> float:
    """IoU é£æ ¼äº¤é›†ï¼šsum(min)/sum(max)ã€‚ä½¿ç”¨å¯†åº¦ç›´æ–¹å›¾ã€‚"""
    h1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    h2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    return float(np.sum(np.minimum(h1, h2)) / np.sum(np.maximum(h1, h2)))


def estimate_stable_size_vs_full(area_values: np.ndarray,
                                 threshold: float = STAB_THRESHOLD,
                                 step: int = STAB_STEP,
                                 consecutive: int = STAB_CONSECUTIVE,
                                 repeats: int = STAB_REPEATS) -> int | None:
    """åŸºäºä¸å…¨é‡æ›²çº¿çš„ IoU ç›¸ä¼¼åº¦ï¼Œä¼°è®¡ç¨³å®šæ ·æœ¬é‡ã€‚

    - åœ¨æ¯ä¸ªæ ·æœ¬é‡ nï¼ˆæ­¥é•¿ stepï¼‰ä¸‹ï¼Œé‡å¤æŠ½æ · repeats æ¬¡ï¼Œä¸å…¨é‡ç›´æ–¹å›¾æ¯”è¾ƒ IoUï¼Œå–å¹³å‡ï¼›
    - è¿”å›ç¬¬ä¸€ä¸ªè¿ç»­ consecutive ä¸ªç‚¹å‡ >= threshold çš„æœ€å° nï¼›è‹¥æ‰¾ä¸åˆ°åˆ™è¿”å› Noneã€‚
    """
    if area_values.size < step * 2:
        return None
    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    full_hist, _ = np.histogram(area_values, bins=bins, density=True)

    sims: list[float] = []
    ns: list[int] = []
    max_n = area_values.size
    for n in range(step, max_n + 1, step):
        vals = []
        for _ in range(repeats):
            idx = np.random.choice(area_values.size, size=n, replace=False)
            sampled = area_values[idx]
            hist, _ = np.histogram(sampled, bins=bins, density=True)
            vals.append(compute_hist_iou(hist, full_hist))
        sims.append(float(np.mean(vals)))
        ns.append(n)

        # æ£€æŸ¥æ˜¯å¦å·²æ»¡è¶³è¿ç»­æ¡ä»¶
        if len(sims) >= consecutive and all(x >= threshold for x in sims[-consecutive:]):
            return ns[-consecutive]

    # å°è¯•è¿”å›è¾¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦çš„ nï¼ˆè‹¥æœªè¾¾é˜ˆå€¼ï¼‰
    if sims:
        best_idx = int(np.argmax(sims))
        return ns[best_idx]
    return None


def main():
    # å¯å¤ç°æ€§
    np.random.seed(42)

    out_dir = get_writable_output_dir(RESULT_DIR, min_free_mb=50)
    ensure_dir(out_dir)

    # é¢„å…ˆæ„å»ºæ‰€æœ‰ç»„çš„å®Œæ•´ç›´æ–¹å›¾ï¼ˆä½œä¸ºè¢«æ¯”è¾ƒå¯¹è±¡ï¼‰
    target_histograms: dict[str, np.ndarray] = {}
    for day in ALL_GROUPS_DAYS:
        for data_folder in ALL_GROUPS_DATAFOLDERS:
            key = f"{day}_{data_folder}"
            area = load_area_series(DATASET1_BASE, day, data_folder)
            if area is None:
                continue
            hist, bins = build_full_hist(area)
            target_histograms[key] = hist

    if not target_histograms:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„ç›®æ ‡ç›´æ–¹å›¾ï¼Œè¯·æ£€æŸ¥ DATASET1 è·¯å¾„ä¸æ•°æ®ã€‚")
        return
    print(f"âœ… DATASET1 å¯å¯¹æ¯”æ›²çº¿æ•°ï¼š{len(target_histograms)}")

    # è¯»å–ç¨³å®šæ€§æ˜ å°„ï¼ˆå¯é€‰ï¼‰
    stability_map, global_75th = load_stability_map(STABILITY_XLSX) if USE_STABILITY else ({}, None)
    if USE_STABILITY:
        print(f"ğŸ”— ä½¿ç”¨ç¨³å®šæ€§ç­–ç•¥ï¼š{STABILITY_STRATEGY}")
        if global_75th is not None:
            print(f"   - å…¨å±€75åˆ†ä½æ¨èï¼š{global_75th}")

    # é’ˆå¯¹æ¯ä¸ªå‚è€ƒç»„ã€æ¯ä¸ªæŠ½æ ·é‡ï¼ˆæˆ–ç¨³å®šæ€§ç­–ç•¥ï¼‰ï¼Œè¿›è¡Œä¸€æ¬¡å†…éƒ¨åŒ¹é…
    for day in ALL_GROUPS_DAYS:
        for data_folder in ALL_GROUPS_DATAFOLDERS:
            ref_key = f"{day}_{data_folder}"
            area = load_area_series(DATASET1_BASE, day, data_folder)
            if area is None:
                print(f"âš ï¸ è·³è¿‡ {ref_key}ï¼ˆæ•°æ®ä¸è¶³æˆ–æœªæ‰¾åˆ°ï¼‰ã€‚")
                continue

            # å†³å®šæœ¬æ¬¡è¦æµ‹è¯•çš„æŠ½æ ·é‡åˆ—è¡¨
            if USE_STABILITY:
                if STABILITY_MODE == 'from_excel':
                    # æ ¹æ®ç­–ç•¥ç”Ÿæˆæ ·æœ¬é‡åˆ—è¡¨ï¼ˆä»¥ä¾¿ä¸å¤šæ¡£å›ºå®šé‡å¹¶è¡Œè¾“å‡ºä¸€è‡´ï¼Œä»ä½¿ç”¨åˆ—è¡¨ï¼‰
                    if STABILITY_STRATEGY == 'per_file':
                        ss = stability_map.get(ref_key, None)
                        sample_sizes_to_use = [ss] if ss and ss > 0 else []
                        if not sample_sizes_to_use:
                            print(f"   â†’ {ref_key} æ— ç¨³å®šç‚¹ï¼Œå›é€€ä½¿ç”¨å›ºå®šSAMPLE_SIZESã€‚")
                            sample_sizes_to_use = SAMPLE_SIZES
                    elif STABILITY_STRATEGY == 'global_75th':
                        sample_sizes_to_use = [global_75th] if global_75th else SAMPLE_SIZES
                    elif STABILITY_STRATEGY == 'global_90th':
                        sample_sizes_to_use = [int(np.percentile(list(stability_map.values()), 90))] if stability_map else SAMPLE_SIZES
                    elif STABILITY_STRATEGY == 'global_max':
                        sample_sizes_to_use = [max(stability_map.values())] if stability_map else SAMPLE_SIZES
                    else:
                        sample_sizes_to_use = SAMPLE_SIZES
                else:  # vs_full æ¨¡å¼ï¼šä¸å…¨é‡æ›²çº¿æ¯”è¾ƒä¼°è®¡ç¨³å®šæ ·æœ¬é‡
                    ss = estimate_stable_size_vs_full(area)
                    if ss is None:
                        print(f"   â†’ {ref_key} æœªä¼°è®¡å‡ºç¨³å®šæ ·æœ¬é‡ï¼Œå›é€€å›ºå®šSAMPLE_SIZESã€‚")
                        sample_sizes_to_use = SAMPLE_SIZES
                    else:
                        sample_sizes_to_use = [ss]
                        print(f"   â†’ {ref_key} vs_full ç¨³å®šæ ·æœ¬é‡ä¼°è®¡ï¼š{ss}")
            else:
                sample_sizes_to_use = SAMPLE_SIZES

            for sample_size in sample_sizes_to_use:
                ref_hist, bin_centers = build_hist_from_sample(area, sample_size)
                if ref_hist is None:
                    print(f"âš ï¸ {ref_key} å°‘äº {sample_size} ä¸ªç»†èƒï¼Œè·³è¿‡è¯¥æŠ½æ ·é‡ã€‚")
                    continue

                # è®¡ç®—æ‰€æœ‰ç›®æ ‡ä¸å½“å‰å‚è€ƒçš„å¤šæŒ‡æ ‡ç›¸ä¼¼åº¦
                records = []
                for tgt_key, tgt_hist in target_histograms.items():
                    row = {"Compared Folder": tgt_key}
                    for method in SIMILARITY_METHODS:
                        val = compute_similarity(ref_hist, tgt_hist, method, bin_centers)
                        row[DISPLAY_NAMES[method]] = val
                    records.append(row)

                result_df = pd.DataFrame(records)

                # ä¿å­˜æ’åºç»“æœï¼ˆExcel -> CSV å›é€€ï¼‰
                base_name = f"intra_ds1_{ref_key}_N{sample_size}".replace(':', '_')
                excel_path = os.path.join(out_dir, f"{base_name}.xlsx")
                try:
                    with pd.ExcelWriter(excel_path) as writer:
                        for method in SIMILARITY_METHODS:
                            col = DISPLAY_NAMES[method]
                            ascending = False if method in HIGHER_BETTER else True
                            result_df.sort_values(by=col, ascending=ascending).to_excel(writer, sheet_name=method, index=False)
                    excel_saved = True
                except OSError as e:
                    excel_saved = False
                    print(f"âš ï¸ å†™å…¥ Excel å¤±è´¥ï¼š{e}ã€‚å¯¼å‡ºä¸º CSVã€‚")
                    for method in SIMILARITY_METHODS:
                        col = DISPLAY_NAMES[method]
                        ascending = False if method in HIGHER_BETTER else True
                        csv_path = os.path.join(out_dir, f"{base_name}_{method}.csv")
                        result_df.sort_values(by=col, ascending=ascending).to_csv(csv_path, index=False)

                # æ§åˆ¶å°è¾“å‡º TOP_Kï¼ˆæŒ‰ PLOT_METHODï¼‰
                col_plot = DISPLAY_NAMES[PLOT_METHOD]
                ascending_plot = False if PLOT_METHOD in HIGHER_BETTER else True
                print(f"\nğŸ“Œ å‚è€ƒ {ref_key} | N={sample_size} | æ’åºä¾æ®ï¼š{col_plot}")
                tmp = result_df.sort_values(by=col_plot, ascending=ascending_plot).head(TOP_K)
                for i, (_, row) in enumerate(tmp.iterrows(), start=1):
                    print(f"{i}. {row['Compared Folder']}  |  {col_plot}={row[col_plot]:.4f}")

                # ç»˜åˆ¶å‚è€ƒ vs TOP_K å¯¹æ¯”æ›²çº¿
                x_bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
                x = 0.5 * (x_bins[:-1] + x_bins[1:])
                plt.figure(figsize=(14, 7))
                ref_smooth = gaussian_filter1d(ref_hist, sigma=2)
                plt.plot(x, ref_smooth, label=f"Ref {ref_key} (N={sample_size})", color="black", linewidth=2, zorder=10)

                colors = plt.cm.tab20(np.linspace(0, 1, TOP_K))
                for color, (_, row) in zip(colors, tmp.iterrows()):
                    tgt_hist = target_histograms[row['Compared Folder']]
                    tgt_smooth = gaussian_filter1d(tgt_hist, sigma=2)
                    plt.plot(x, tgt_smooth, label=f"{row['Compared Folder']} ({row[col_plot]:.3f})", color=color, alpha=0.9)

                plt.title(f"DATASET1 Intra Similarity ({col_plot}) | Ref={ref_key} N={sample_size}")
                plt.xlabel("Cell Area")
                plt.ylabel("Normalized Frequency")
                plt.legend(fontsize=8, ncol=2)
                plt.tight_layout()
                fig_path = os.path.join(out_dir, f"{base_name}_top_matches.png")
                try:
                    plt.savefig(fig_path, dpi=300, bbox_inches="tight")
                    plt.close()
                except OSError as e:
                    print(f"âš ï¸ ä¿å­˜å¯¹æ¯”å›¾å¤±è´¥ï¼š{e}")

                # ç”Ÿæˆä¸¤ç±»èšç±»çƒ­åŠ›å›¾
                try:
                    similarity_metrics = [
                        DISPLAY_NAMES['cosine'],
                        DISPLAY_NAMES['pearson'],
                        DISPLAY_NAMES['intersection'],
                    ]
                    distance_metrics = [
                        DISPLAY_NAMES['chi_square'],
                        DISPLAY_NAMES['kl'],
                        DISPLAY_NAMES['wasserstein'],
                    ]

                    folders = result_df['Compared Folder']
                    sim_part = result_df[similarity_metrics]
                    dist_part = result_df[distance_metrics]

                    sim_scaled = pd.DataFrame(
                        MinMaxScaler().fit_transform(sim_part),
                        columns=sim_part.columns,
                        index=folders,
                    )
                    dist_scaled = pd.DataFrame(
                        MinMaxScaler().fit_transform(dist_part),
                        columns=dist_part.columns,
                        index=folders,
                    )

                    g1 = sns.clustermap(sim_scaled, cmap="Reds", annot=True, fmt=".2f", figsize=(10, 7), metric="euclidean", method="ward")
                    g1.fig.suptitle(f"Similarity Clustering | Ref={ref_key} N={sample_size}")
                    heat1_path = os.path.join(out_dir, f"{base_name}_similarity_clustermap.png")
                    g1.savefig(heat1_path, dpi=300, bbox_inches="tight")
                    plt.close(g1.fig)

                    g2 = sns.clustermap(dist_scaled, cmap="Blues_r", annot=True, fmt=".2f", figsize=(10, 7), metric="euclidean", method="ward")
                    g2.fig.suptitle(f"Distance Clustering | Ref={ref_key} N={sample_size}")
                    heat2_path = os.path.join(out_dir, f"{base_name}_distance_clustermap.png")
                    g2.savefig(heat2_path, dpi=300, bbox_inches="tight")
                    plt.close(g2.fig)
                except OSError as e:
                    print(f"âš ï¸ ä¿å­˜çƒ­åŠ›å›¾å¤±è´¥ï¼š{e}")

                # è®°å½•å°ç»“
                if 'excel_path' in locals() and os.path.exists(excel_path):
                    print(f"ğŸ“ æ’åºç»“æœï¼š{excel_path}")
                else:
                    print(f"ğŸ“ æ’åºç»“æœï¼š{out_dir} ä¸‹ {base_name}_<method>.csv")
                if os.path.exists(fig_path):
                    print(f"ğŸ–¼ï¸ å¯¹æ¯”å›¾ï¼š{fig_path}")


if __name__ == "__main__":
    main()


