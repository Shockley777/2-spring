import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.ndimage import gaussian_filter1d
from scipy.stats import wasserstein_distance, pearsonr, entropy
from sklearn.metrics.pairwise import cosine_similarity
import shutil
import tempfile
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler


# å…¨å±€å‚æ•°
DATASET1_BASE = r"D:\project\2-spring\DATASET1\data"
DATASET2_BASE = r"D:\project\2-spring\DATASET2\data"
AREA_COL = "area"
BINS = 50
RANGE = (500, 3500)
REF_RATIO = 1.0  # å‚è€ƒåº“ä½¿ç”¨å…¨éƒ¨æ•°æ®
TARGET_RATIO = 0.8  # ç›®æ ‡æ›²çº¿æŠ½æ ·æ¯”ä¾‹ï¼Œå¯æŒ‰éœ€è°ƒæ•´
TOP_K = 10
RESULT_DIR = os.path.join("similarity", "results", "cross_dataset")

# å¯é€‰ï¼šæ‰‹åŠ¨æŒ‡å®šç›®æ ‡æ›²çº¿é”®ï¼Œä¾‹å¦‚ 'DS2::DAY4_data1'ï¼›ä¸º None æ—¶éšæœºé€‰æ‹©
TARGET_KEY = None

# å¯é€‰ï¼šç»˜å›¾ä¸TOPKé€‰æ‹©æ‰€ä¾æ®çš„æŒ‡æ ‡
PLOT_METHOD = 'intersection'  # å¯é€‰ï¼šintersection/cosine/pearson/chi_square/kl/wasserstein

# å®šä¹‰ç›¸ä¼¼åº¦/è·ç¦»æ–¹æ³•é›†åˆ
SIMILARITY_METHODS = [
    'intersection',    # ç›´æ–¹å›¾äº¤é›†ï¼Œç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    'cosine',          # ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    'pearson',         # çš®å°”é€Šç›¸å…³ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
    'chi_square',      # å¡æ–¹è·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    'kl',              # KLæ•£åº¦ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    'wasserstein',     # Wassersteinè·ç¦»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
]

# ç”¨äºæ‰“å°ä¸ä¿å­˜æ—¶çš„äººç±»å¯è¯»åç§°
DISPLAY_NAMES = {
    'intersection': 'Histogram Intersection',
    'cosine': 'Cosine Similarity',
    'pearson': 'Pearson Correlation',
    'chi_square': 'Chi-Square Distance',
    'kl': 'KL Divergence',
    'wasserstein': 'Wasserstein Distance',
}

# å“ªäº›æŒ‡æ ‡è¶Šå¤§è¶Šå¥½
HIGHER_BETTER = {'intersection', 'cosine', 'pearson'}


def get_writable_output_dir(preferred_dir: str, min_free_mb: int = 100) -> str:
    """é€‰æ‹©å¯å†™ä¸”æœ‰è¶³å¤Ÿå‰©ä½™ç©ºé—´çš„è¾“å‡ºç›®å½•ã€‚

    é€‰æ‹©é¡ºåºï¼š
    1) ç¯å¢ƒå˜é‡ CROSS_DATASET_OUTDIR æŒ‡å®šè·¯å¾„ï¼ˆè‹¥æœ‰è¶³å¤Ÿç©ºé—´ï¼‰
    2) preferred_dirï¼ˆè‹¥æœ‰è¶³å¤Ÿç©ºé—´ï¼‰
    3) ç³»ç»Ÿä¸´æ—¶ç›®å½•ä¸‹çš„ cross_dataset å­ç›®å½•
    è‹¥éƒ½ä¸è¶³ï¼Œåˆ™ä»è¿”å›ä¸´æ—¶ç›®å½•ï¼ˆä½†ä¼šæ‰“å°è­¦å‘Šï¼‰ã€‚
    """
    # 1) ç¯å¢ƒå˜é‡ä¼˜å…ˆ
    env_dir = os.environ.get("CROSS_DATASET_OUTDIR")
    candidates = []
    if env_dir:
        candidates.append(env_dir)
    candidates.append(preferred_dir)
    candidates.append(os.path.join(tempfile.gettempdir(), "cross_dataset"))

    for path in candidates:
        try:
            os.makedirs(path, exist_ok=True)
            total, used, free = shutil.disk_usage(path)
            if free >= min_free_mb * 1024 * 1024:
                return path
        except Exception:
            continue

    # æ‰€æœ‰å€™é€‰éƒ½ä¸è¶³ï¼Œè¿”å›æœ€åä¸€ä¸ªå¹¶æç¤º
    fallback = candidates[-1]
    try:
        os.makedirs(fallback, exist_ok=True)
    except Exception:
        pass
    print("âš ï¸ æ‰€æœ‰è¾“å‡ºç›®å½•ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå°†å°è¯•å†™å…¥ä¸´æ—¶ç›®å½•ï¼Œå¯èƒ½ä»ä¼šå¤±è´¥ã€‚")
    return fallback


def ensure_dir(path: str) -> None:
    if not os.path.exists(path):
        os.makedirs(path)


def load_histogram(base_dir: str, day_folder: str, data_folder: str, ratio: float):
    csv_path = os.path.join(base_dir, day_folder, data_folder, "total", "merged.csv")
    if not os.path.exists(csv_path):
        return None, None

    df = pd.read_csv(csv_path)
    if AREA_COL not in df.columns:
        return None, None

    area_series = df[AREA_COL].dropna().values
    area_series = area_series[(area_series >= RANGE[0]) & (area_series <= RANGE[1])]
    if area_series.size < 5:
        return None, None

    np.random.shuffle(area_series)
    sampled = area_series[: int(area_series.size * ratio)] if 0 < ratio < 1 else area_series

    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(sampled, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


def compute_histogram_intersection(hist1: np.ndarray, hist2: np.ndarray) -> float:
    h1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    h2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    h1 /= np.sum(h1)
    h2 /= np.sum(h2)
    return float(np.sum(np.minimum(h1, h2)))


def compute_similarity(hist1: np.ndarray, hist2: np.ndarray, method: str, bin_centers: np.ndarray) -> float:
    """ç»Ÿä¸€çš„ç›¸ä¼¼åº¦/è·ç¦»è®¡ç®—å…¥å£ã€‚

    è¯´æ˜ï¼šé™¤ 'intersection' å¤–ï¼Œå…¶ä½™æ–¹æ³•å‡åŸºäºå½’ä¸€åŒ–ç›´æ–¹å›¾ï¼›
    è·ç¦»å‹æŒ‡æ ‡ï¼ˆchi_square/kl/wassersteinï¼‰è¶Šå°è¶Šç›¸ä¼¼ã€‚
    """
    h1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    h2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    h1 /= np.sum(h1)
    h2 /= np.sum(h2)

    if method == 'intersection':
        return float(np.sum(np.minimum(h1, h2)))
    elif method == 'cosine':
        return float(cosine_similarity(h1.reshape(1, -1), h2.reshape(1, -1))[0, 0])
    elif method == 'pearson':
        corr, _ = pearsonr(h1, h2)
        return float(0 if np.isnan(corr) else corr)
    elif method == 'chi_square':
        denom = h1 + h2
        return float(0.5 * np.sum(((h1 - h2) ** 2) / (denom)))
    elif method == 'kl':
        return float(entropy(h1, h2))
    elif method == 'wasserstein':
        return float(wasserstein_distance(bin_centers, bin_centers, h1, h2))
    else:
        raise ValueError(f"Unsupported similarity method: {method}")


def build_reference_library() -> dict:
    days_ds1 = [f"DAY{i}" for i in range(1, 7)]
    data_folders_ds1 = [f"data{j}" for j in range(1, 6)]

    reference_histograms = {}
    for day in days_ds1:
        for data_folder in data_folders_ds1:
            key = f"DS1::{day}_{data_folder}"
            hist, bins = load_histogram(DATASET1_BASE, day, data_folder, REF_RATIO)
            if hist is not None:
                reference_histograms[key] = hist
    return reference_histograms


def gather_dataset2_targets() -> dict:
    days_ds2 = [f"DAY{i}" for i in range(2, 8)]
    data_folders_ds2 = [f"data{j}" for j in range(1, 7)]

    target_histograms = {}
    for day in days_ds2:
        for data_folder in data_folders_ds2:
            key = f"DS2::{day}_{data_folder}"
            hist, bins = load_histogram(DATASET2_BASE, day, data_folder, TARGET_RATIO)
            if hist is not None:
                target_histograms[key] = hist
    return target_histograms


def main():
    # ä»…å›ºå®š numpy çš„éšæœºæ€§ä»¥ä¿è¯ç›´æ–¹å›¾æŠ½æ ·å¯å¤ç°
    np.random.seed(42)
    # é€‰æ‹©è¾“å‡ºç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡/æ—¢å®šç›®å½•ï¼Œç©ºé—´ä¸è¶³åˆ™è‡ªåŠ¨å›é€€åˆ°ä¸´æ—¶ç›®å½•
    out_dir = get_writable_output_dir(RESULT_DIR, min_free_mb=50)
    ensure_dir(out_dir)

    # 1) æ„å»º DATASET1 å‚è€ƒåº“
    reference_histograms = build_reference_library()
    if not reference_histograms:
        print("âŒ æœªèƒ½ä» DATASET1 æ„å»ºä»»ä½•å‚è€ƒç›´æ–¹å›¾ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„ä¸æ–‡ä»¶ã€‚")
        return
    print(f"âœ… å‚è€ƒåº“è½½å…¥æˆåŠŸï¼š{len(reference_histograms)} æ¡æ›²çº¿")

    # 2) æ”¶é›† DATASET2 å¯ä½œä¸ºç›®æ ‡çš„æ›²çº¿åˆ—è¡¨ï¼Œå¹¶éšæœºé€‰æ‹©ä¸€æ¡
    target_histograms = gather_dataset2_targets()
    if not target_histograms:
        print("âŒ æœªèƒ½ä» DATASET2 è½½å…¥ä»»ä½•ç›®æ ‡ç›´æ–¹å›¾ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„ä¸æ–‡ä»¶ã€‚")
        return
    if TARGET_KEY and TARGET_KEY in target_histograms:
        target_key = TARGET_KEY
    else:
        # ä½¿ç”¨ç³»ç»Ÿç†µæºï¼Œé¿å…å—å…¨å±€éšæœºç§å­å½±å“
        target_key = random.SystemRandom().choice(list(target_histograms.keys()))
    target_hist = target_histograms[target_key]
    print(f"ğŸ¯ éšæœºé€‰å–ç›®æ ‡æ›²çº¿ï¼š{target_key}")

    # 3) è®¡ç®—ä¸å‚è€ƒåº“æ‰€æœ‰æ›²çº¿çš„å¤šæŒ‡æ ‡ç›¸ä¼¼åº¦/è·ç¦»
    x_bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    x = 0.5 * (x_bins[:-1] + x_bins[1:])
    records = []
    for ref_key, ref_hist in reference_histograms.items():
        row = {"Reference": ref_key}
        for method in SIMILARITY_METHODS:
            val = compute_similarity(target_hist, ref_hist, method, x)
            row[DISPLAY_NAMES[method]] = val
        records.append(row)

    result_df = pd.DataFrame(records)

    # 4) ä¿å­˜åˆ° Excelï¼ˆæ¯ä¸ªæŒ‡æ ‡ä¸€ä¸ª sheetï¼ŒæŒ‰å¯¹åº”æ–¹å‘æ’åºï¼‰ï¼›ç£ç›˜ä¸è¶³æ—¶å›é€€ä¸º CSV
    excel_path = os.path.join(out_dir, f"match_results_{target_key.replace('::','_')}.xlsx")
    try:
        with pd.ExcelWriter(excel_path) as writer:
            for method in SIMILARITY_METHODS:
                col = DISPLAY_NAMES[method]
                ascending = False if method in HIGHER_BETTER else True
                result_df.sort_values(by=col, ascending=ascending).to_excel(writer, sheet_name=method, index=False)
        excel_saved = True
    except OSError as e:
        excel_saved = False
        print(f"âš ï¸ å†™å…¥ Excel å¤±è´¥ï¼š{e}. å°†æ”¹ä¸ºå¯¼å‡º CSV æ–‡ä»¶ã€‚")
        for method in SIMILARITY_METHODS:
            col = DISPLAY_NAMES[method]
            ascending = False if method in HIGHER_BETTER else True
            csv_path = os.path.join(out_dir, f"match_results_{method}_{target_key.replace('::','_')}.csv")
            result_df.sort_values(by=col, ascending=ascending).to_csv(csv_path, index=False)

    # 5) ç»ˆç«¯è¾“å‡ºæ¯ä¸ªæŒ‡æ ‡çš„ TOP_K
    for method in SIMILARITY_METHODS:
        col = DISPLAY_NAMES[method]
        ascending = False if method in HIGHER_BETTER else True
        print(f"\nğŸ” {col} TOP{TOP_K}ï¼š")
        tmp = result_df.sort_values(by=col, ascending=ascending).head(TOP_K)
        for i, (_, row) in enumerate(tmp.iterrows(), start=1):
            print(f"{i}. {row['Reference']}  |  {col}={row[col]:.4f}")

    # 6) ç»˜åˆ¶ç›®æ ‡æ›²çº¿ä¸PLOT_METHODå¯¹åº”çš„TOP_Kå‚è€ƒæ›²çº¿å¯¹æ¯”å›¾
    plt.figure(figsize=(14, 7))
    target_smooth = gaussian_filter1d(target_hist, sigma=2)
    plt.plot(x, target_smooth, label=f"Target {target_key}", color="black", linewidth=2, zorder=10)

    colors = plt.cm.tab20(np.linspace(0, 1, TOP_K))
    col_plot = DISPLAY_NAMES[PLOT_METHOD]
    ascending_plot = False if PLOT_METHOD in HIGHER_BETTER else True
    top_rows = result_df.sort_values(by=col_plot, ascending=ascending_plot).head(TOP_K)
    for color, (_, row) in zip(colors, top_rows.iterrows()):
        ref_hist = reference_histograms[row['Reference']]
        ref_smooth = gaussian_filter1d(ref_hist, sigma=2)
        plt.plot(x, ref_smooth, label=f"{row['Reference']} ({row[col_plot]:.3f})", color=color, alpha=0.9)

    plt.title(f"Cross-Dataset Similarity ({DISPLAY_NAMES[PLOT_METHOD]}): Target vs Top Matches")
    plt.xlabel("Cell Area")
    plt.ylabel("Normalized Frequency")
    plt.legend(fontsize=8, ncol=2)
    plt.tight_layout()
    fig_path = os.path.join(out_dir, f"match_plot_{target_key.replace('::','_')}.png")
    try:
        plt.savefig(fig_path, dpi=300, bbox_inches="tight")
        plt.close()
        fig_saved = True
    except OSError as e:
        fig_saved = False
        print(f"âš ï¸ ä¿å­˜å›¾ç‰‡å¤±è´¥ï¼š{e}ã€‚")

    # 7) ç”Ÿæˆèšç±»çƒ­åŠ›å›¾ï¼ˆç›¸ä¼¼åº¦æŒ‡æ ‡ä¸è·ç¦»æŒ‡æ ‡ï¼‰
    try:
        similarity_metrics = [
            DISPLAY_NAMES['cosine'],
            DISPLAY_NAMES['pearson'],
            DISPLAY_NAMES['intersection'],
        ]
        distance_metrics = [
            DISPLAY_NAMES['chi_square'],
            DISPLAY_NAMES['kl'],
            DISPLAY_NAMES['wasserstein'],
        ]

        folders = result_df['Reference']
        # ç›¸ä¼¼åº¦ç±»ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        sim_part = result_df[similarity_metrics]
        sim_scaler = MinMaxScaler()
        sim_scaled = pd.DataFrame(
            sim_scaler.fit_transform(sim_part),
            columns=sim_part.columns,
            index=folders,
        )
        g1 = sns.clustermap(
            sim_scaled,
            cmap="Reds",
            annot=True,
            fmt=".2f",
            figsize=(10, 7),
            metric="euclidean",
            method="ward",
        )
        g1.fig.suptitle(f"Similarity Clustering to {target_key} (High=Better)")
        heat1_path = os.path.join(out_dir, f"clustering_similarity_metrics_{target_key.replace('::','_')}.png")
        g1.savefig(heat1_path, dpi=300, bbox_inches="tight")
        plt.close(g1.fig)

        # è·ç¦»ç±»ï¼ˆè¶Šå°è¶Šå¥½ï¼‰ï¼Œä¸ºäº†é¢œè‰²ä¸€è‡´æ€§å¯åšåå‘æˆ–ç›´æ¥å±•ç¤º
        dist_part = result_df[distance_metrics]
        dist_scaler = MinMaxScaler()
        dist_scaled = pd.DataFrame(
            dist_scaler.fit_transform(dist_part),
            columns=dist_part.columns,
            index=folders,
        )
        g2 = sns.clustermap(
            dist_scaled,
            cmap="Blues_r",
            annot=True,
            fmt=".2f",
            figsize=(10, 7),
            metric="euclidean",
            method="ward",
        )
        g2.fig.suptitle(f"Distance Clustering to {target_key} (Low=Better)")
        heat2_path = os.path.join(out_dir, f"clustering_distance_metrics_{target_key.replace('::','_')}.png")
        g2.savefig(heat2_path, dpi=300, bbox_inches="tight")
        plt.close(g2.fig)
        heatmaps_saved = True
    except OSError as e:
        heatmaps_saved = False
        print(f"âš ï¸ ä¿å­˜çƒ­åŠ›å›¾å¤±è´¥ï¼š{e}ã€‚")

    if excel_saved:
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜ï¼š")
        print(f"- {excel_path}")
    else:
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜ï¼ˆCSV å›é€€ï¼‰ï¼š")
        print(f"- {out_dir} ä¸‹å„ä¸ª match_results_<method>_{target_key.replace('::','_')}.csv")
    if 'fig_saved' in locals() and fig_saved:
        print(f"- {fig_path}")
    else:
        print("- æ›²çº¿å¯¹æ¯”å›¾æœªä¿å­˜ï¼ˆç£ç›˜ç©ºé—´ä¸è¶³æˆ–IOé”™è¯¯ï¼‰")
    if 'heatmaps_saved' in locals() and heatmaps_saved:
        print(f"- {heat1_path}")
        print(f"- {heat2_path}")
    else:
        print("- çƒ­åŠ›å›¾æœªä¿å­˜ï¼ˆç£ç›˜ç©ºé—´ä¸è¶³æˆ–IOé”™è¯¯ï¼‰")


if __name__ == "__main__":
    main()


