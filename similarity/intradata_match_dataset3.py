import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.ndimage import gaussian_filter1d
from scipy.stats import wasserstein_distance, pearsonr, entropy
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
import shutil
import tempfile


# åŸºæœ¬å‚æ•°ï¼ˆDATASET3ï¼‰
DATASET3_BASE = r"D:\project\2-spring\DATASET3\data"
AREA_COL = "area"
BINS = 50
RANGE = (500, 3500)
# é€’å½’æ‰«æï¼Œæ— éœ€é¢„è®¾ DAYX/DATA ç»“æ„
SAMPLE_SIZES = [2500, 3000, 3500, 4000, 4500, 5000]
TOP_K = 10
RESULT_DIR = os.path.join("similarity", "results", "intra_dataset3")

# ç¨³å®šæ€§æ¥å…¥ï¼ˆå¯é€‰ï¼‰
USE_STABILITY = True
STABILITY_XLSX = r"D:\project\2-spring\DATASET3\code\stability\dataset3_stability_analysis.xlsx"
# 'per_file' | 'global_75th' | 'global_90th' | 'global_max'
STABILITY_STRATEGY = 'per_file'

# æŒ‡æ ‡è®¾ç½®
SIMILARITY_METHODS = [
    'intersection',    # è¶Šå¤§è¶Šå¥½
    'cosine',          # è¶Šå¤§è¶Šå¥½
    'pearson',         # è¶Šå¤§è¶Šå¥½
    'chi_square',      # è¶Šå°è¶Šå¥½
    'kl',              # è¶Šå°è¶Šå¥½
    'wasserstein',     # è¶Šå°è¶Šå¥½
]
DISPLAY_NAMES = {
    'intersection': 'Histogram Intersection',
    'cosine': 'Cosine Similarity',
    'pearson': 'Pearson Correlation',
    'chi_square': 'Chi-Square Distance',
    'kl': 'KL Divergence',
    'wasserstein': 'Wasserstein Distance',
}
HIGHER_BETTER = {'intersection', 'cosine', 'pearson'}
PLOT_METHOD = 'intersection'


def ensure_dir(path: str) -> None:
    if not os.path.exists(path):
        os.makedirs(path)


def get_writable_output_dir(preferred_dir: str, min_free_mb: int = 50) -> str:
    env_dir = os.environ.get("INTRA_DATASET3_OUTDIR")
    candidates = []
    if env_dir:
        candidates.append(env_dir)
    candidates.append(preferred_dir)
    candidates.append(os.path.join(tempfile.gettempdir(), "intra_dataset3"))

    for path in candidates:
        try:
            os.makedirs(path, exist_ok=True)
            total, used, free = shutil.disk_usage(path)
            if free >= min_free_mb * 1024 * 1024:
                return path
        except Exception:
            continue

    fallback = candidates[-1]
    try:
        os.makedirs(fallback, exist_ok=True)
    except Exception:
        pass
    print("âš ï¸ æ‰€æœ‰è¾“å‡ºç›®å½•ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå°†å°è¯•å†™å…¥ä¸´æ—¶ç›®å½•ï¼Œå¯èƒ½ä»ä¼šå¤±è´¥ã€‚")
    return fallback


def iter_merged_csvs(base_dir: str):
    for root, dirs, files in os.walk(base_dir):
        if 'total' + os.sep in root or root.endswith(os.sep + 'total'):
            if 'merged.csv' in files:
                yield os.path.join(root, 'merged.csv')


def path_to_key(file_path: str, base_dir: str) -> str | None:
    try:
        # å–åˆ° total çš„ä¸Šä¸€çº§ç›®å½•ä½œä¸ºç»„ç›®å½•
        parent_dir = os.path.dirname(os.path.dirname(file_path))  # .../<group>/total/merged.csv -> .../<group>
        rel = os.path.relpath(parent_dir, base_dir)
        # å°†å¤šçº§ç›¸å¯¹è·¯å¾„æ‹¼æˆ key
        return rel.replace(os.sep, '_')
    except Exception:
        return None


def load_area_series(csv_path: str) -> np.ndarray | None:
    if not os.path.exists(csv_path):
        return None
    df = pd.read_csv(csv_path)
    if AREA_COL not in df.columns:
        return None
    area_series = df[AREA_COL].dropna().values
    area_series = area_series[(area_series >= RANGE[0]) & (area_series <= RANGE[1])]
    if area_series.size < 5:
        return None
    return area_series


def build_hist_from_sample(area_values: np.ndarray, sample_size: int) -> tuple[np.ndarray, np.ndarray] | tuple[None, None]:
    if area_values.size < sample_size:
        return None, None
    idx = np.random.choice(area_values.size, size=sample_size, replace=False)
    sampled = area_values[idx]
    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(sampled, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


def build_full_hist(area_values: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(area_values, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


def compute_similarity(hist1: np.ndarray, hist2: np.ndarray, method: str, bin_centers: np.ndarray) -> float:
    h1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    h2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    h1 /= np.sum(h1)
    h2 /= np.sum(h2)
    if method == 'intersection':
        return float(np.sum(np.minimum(h1, h2)))
    elif method == 'cosine':
        return float(cosine_similarity(h1.reshape(1, -1), h2.reshape(1, -1))[0, 0])
    elif method == 'pearson':
        corr, _ = pearsonr(h1, h2)
        return float(0 if np.isnan(corr) else corr)
    elif method == 'chi_square':
        denom = h1 + h2
        return float(0.5 * np.sum(((h1 - h2) ** 2) / denom))
    elif method == 'kl':
        return float(entropy(h1, h2))
    elif method == 'wasserstein':
        return float(wasserstein_distance(bin_centers, bin_centers, h1, h2))
    else:
        raise ValueError(f"Unsupported similarity method: {method}")


def load_stability_map(xlsx_path: str, base_dir: str) -> tuple[dict[str, int], int] | tuple[dict[str, int], None]:
    if not os.path.exists(xlsx_path):
        print(f"âš ï¸ æœªæ‰¾åˆ°ç¨³å®šæ€§æ–‡ä»¶ï¼š{xlsx_path}ï¼Œå°†ä½¿ç”¨å›ºå®š SAMPLE_SIZESã€‚")
        return {}, None
    try:
        df = pd.read_excel(xlsx_path, sheet_name='è¯¦ç»†ç»“æœ')
    except Exception as e:
        print(f"âš ï¸ è¯»å–ç¨³å®šæ€§æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†ä½¿ç”¨å›ºå®š SAMPLE_SIZESã€‚")
        return {}, None

    stability_map: dict[str, int] = {}
    stable_sizes: list[int] = []
    for _, row in df.iterrows():
        fp = str(row.get('file_path', ''))
        # å°†ç¨³å®šæ€§é‡Œçš„è·¯å¾„è½¬ä¸ºä¸æœ¬è„šæœ¬ä¸€è‡´çš„ key
        try:
            parent_dir = os.path.dirname(os.path.dirname(fp))
            rel = os.path.relpath(parent_dir, base_dir)
            key = rel.replace(os.sep, '_')
        except Exception:
            key = None
        val = row.get('stable_sample_size', None)
        if key and pd.notna(val) and int(val) > 0:
            stability_map[key] = int(val)
            stable_sizes.append(int(val))

    global_75th = int(np.percentile(stable_sizes, 75)) if stable_sizes else None
    return stability_map, global_75th


def main():
    np.random.seed(42)

    out_dir = get_writable_output_dir(RESULT_DIR, min_free_mb=50)
    ensure_dir(out_dir)

    # æ„å»ºæ‰€æœ‰ç›®æ ‡æ›²çº¿ï¼ˆå®Œæ•´ç›´æ–¹å›¾ï¼‰
    target_histograms: dict[str, np.ndarray] = {}
    for csv_path in iter_merged_csvs(DATASET3_BASE):
        key = path_to_key(csv_path, DATASET3_BASE)
        if not key:
            continue
        area = load_area_series(csv_path)
        if area is None:
            continue
        hist, _ = build_full_hist(area)
        target_histograms[key] = hist

    if not target_histograms:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„ç›®æ ‡ç›´æ–¹å›¾ï¼Œè¯·æ£€æŸ¥ DATASET3 è·¯å¾„ä¸æ•°æ®ã€‚")
        return
    print(f"âœ… DATASET3 å¯å¯¹æ¯”æ›²çº¿æ•°ï¼š{len(target_histograms)}")

    stability_map, global_75th = load_stability_map(STABILITY_XLSX, DATASET3_BASE) if USE_STABILITY else ({}, None)
    if USE_STABILITY:
        print(f"ğŸ”— ä½¿ç”¨ç¨³å®šæ€§ç­–ç•¥ï¼š{STABILITY_STRATEGY}")
        if global_75th is not None:
            print(f"   - å…¨å±€75åˆ†ä½æ¨èï¼š{global_75th}")

    # ä»¥æ‰«æåˆ°çš„ key åˆ—è¡¨ä½œä¸ºå‚è€ƒé›†åˆ
    for ref_key, _ in target_histograms.items():
        # åŠ è½½å‚è€ƒç»„ area æ•°æ®
        # åæ¨ csv è·¯å¾„ï¼šå°† key è¿˜åŸæˆç›®å½•è·¯å¾„
        group_dir = os.path.join(DATASET3_BASE, ref_key.replace('_', os.sep))
        csv_path = os.path.join(group_dir, 'total', 'merged.csv')
        area = load_area_series(csv_path)
        if area is None:
            print(f"âš ï¸ è·³è¿‡ {ref_key}ï¼ˆæ•°æ®ä¸è¶³æˆ–æœªæ‰¾åˆ°ï¼‰ã€‚")
            continue

        # å†³å®šä½¿ç”¨çš„æŠ½æ ·é‡
        if USE_STABILITY:
            if STABILITY_STRATEGY == 'per_file':
                ss = stability_map.get(ref_key, None)
                sample_sizes_to_use = [ss] if ss and ss > 0 else []
                if not sample_sizes_to_use:
                    print(f"   â†’ {ref_key} æ— ç¨³å®šç‚¹ï¼Œå›é€€ä½¿ç”¨å›ºå®šSAMPLE_SIZESã€‚")
                    sample_sizes_to_use = SAMPLE_SIZES
            elif STABILITY_STRATEGY == 'global_75th':
                sample_sizes_to_use = [global_75th] if global_75th else SAMPLE_SIZES
            elif STABILITY_STRATEGY == 'global_90th':
                sample_sizes_to_use = [int(np.percentile(list(stability_map.values()), 90))] if stability_map else SAMPLE_SIZES
            elif STABILITY_STRATEGY == 'global_max':
                sample_sizes_to_use = [max(stability_map.values())] if stability_map else SAMPLE_SIZES
            else:
                sample_sizes_to_use = SAMPLE_SIZES
        else:
            sample_sizes_to_use = SAMPLE_SIZES

        for sample_size in sample_sizes_to_use:
            ref_hist, bin_centers = build_hist_from_sample(area, sample_size)
            if ref_hist is None:
                print(f"âš ï¸ {ref_key} å°‘äº {sample_size} ä¸ªç»†èƒï¼Œè·³è¿‡è¯¥æŠ½æ ·é‡ã€‚")
                continue

            # è®¡ç®—å¤šæŒ‡æ ‡ç›¸ä¼¼åº¦
            records = []
            for tgt_key, tgt_hist in target_histograms.items():
                row = {"Compared Folder": tgt_key}
                for method in SIMILARITY_METHODS:
                    val = compute_similarity(ref_hist, tgt_hist, method, bin_centers)
                    row[DISPLAY_NAMES[method]] = val
                records.append(row)

            result_df = pd.DataFrame(records)

            # ä¿å­˜æ’åºï¼ˆExcel -> CSVå›é€€ï¼‰
            base_name = f"intra_ds3_{ref_key}_N{sample_size}".replace(':', '_')
            excel_path = os.path.join(out_dir, f"{base_name}.xlsx")
            try:
                with pd.ExcelWriter(excel_path) as writer:
                    for method in SIMILARITY_METHODS:
                        col = DISPLAY_NAMES[method]
                        ascending = False if method in HIGHER_BETTER else True
                        result_df.sort_values(by=col, ascending=ascending).to_excel(writer, sheet_name=method, index=False)
                excel_saved = True
            except OSError as e:
                excel_saved = False
                print(f"âš ï¸ å†™å…¥ Excel å¤±è´¥ï¼š{e}ã€‚å¯¼å‡ºä¸º CSVã€‚")
                for method in SIMILARITY_METHODS:
                    col = DISPLAY_NAMES[method]
                    ascending = False if method in HIGHER_BETTER else True
                    csv_path = os.path.join(out_dir, f"{base_name}_{method}.csv")
                    result_df.sort_values(by=col, ascending=ascending).to_csv(csv_path, index=False)

            # æ§åˆ¶å°è¾“å‡º TOP_Kï¼ˆæŒ‰ PLOT_METHODï¼‰
            col_plot = DISPLAY_NAMES[PLOT_METHOD]
            ascending_plot = False if PLOT_METHOD in HIGHER_BETTER else True
            print(f"\nğŸ“Œ å‚è€ƒ {ref_key} | N={sample_size} | æ’åºä¾æ®ï¼š{col_plot}")
            tmp = result_df.sort_values(by=col_plot, ascending=ascending_plot).head(TOP_K)
            for i, (_, row) in enumerate(tmp.iterrows(), start=1):
                print(f"{i}. {row['Compared Folder']}  |  {col_plot}={row[col_plot]:.4f}")

            # ç»˜åˆ¶å‚è€ƒ vs TOP_K å¯¹æ¯”æ›²çº¿
            x_bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
            x = 0.5 * (x_bins[:-1] + x_bins[1:])
            plt.figure(figsize=(14, 7))
            ref_smooth = gaussian_filter1d(ref_hist, sigma=2)
            plt.plot(x, ref_smooth, label=f"Ref {ref_key} (N={sample_size})", color="black", linewidth=2, zorder=10)

            colors = plt.cm.tab20(np.linspace(0, 1, TOP_K))
            for color, (_, row) in zip(colors, tmp.iterrows()):
                tgt_hist = target_histograms[row['Compared Folder']]
                tgt_smooth = gaussian_filter1d(tgt_hist, sigma=2)
                plt.plot(x, tgt_smooth, label=f"{row['Compared Folder']} ({row[col_plot]:.3f})", color=color, alpha=0.9)

            plt.title(f"DATASET3 Intra Similarity ({col_plot}) | Ref={ref_key} N={sample_size}")
            plt.xlabel("Cell Area")
            plt.ylabel("Normalized Frequency")
            plt.legend(fontsize=8, ncol=2)
            plt.tight_layout()
            fig_path = os.path.join(out_dir, f"{base_name}_top_matches.png")
            try:
                plt.savefig(fig_path, dpi=300, bbox_inches="tight")
                plt.close()
            except OSError as e:
                print(f"âš ï¸ ä¿å­˜å¯¹æ¯”å›¾å¤±è´¥ï¼š{e}")

            # ç”Ÿæˆä¸¤ç±»èšç±»çƒ­åŠ›å›¾
            try:
                similarity_metrics = [
                    DISPLAY_NAMES['cosine'],
                    DISPLAY_NAMES['pearson'],
                    DISPLAY_NAMES['intersection'],
                ]
                distance_metrics = [
                    DISPLAY_NAMES['chi_square'],
                    DISPLAY_NAMES['kl'],
                    DISPLAY_NAMES['wasserstein'],
                ]

                folders = result_df['Compared Folder']
                sim_part = result_df[similarity_metrics]
                dist_part = result_df[distance_metrics]

                sim_scaled = pd.DataFrame(
                    MinMaxScaler().fit_transform(sim_part),
                    columns=sim_part.columns,
                    index=folders,
                )
                dist_scaled = pd.DataFrame(
                    MinMaxScaler().fit_transform(dist_part),
                    columns=dist_part.columns,
                    index=folders,
                )

                g1 = sns.clustermap(sim_scaled, cmap="Reds", annot=True, fmt=".2f", figsize=(10, 7), metric="euclidean", method="ward")
                g1.fig.suptitle(f"Similarity Clustering | Ref={ref_key} N={sample_size}")
                heat1_path = os.path.join(out_dir, f"{base_name}_similarity_clustermap.png")
                g1.savefig(heat1_path, dpi=300, bbox_inches="tight")
                plt.close(g1.fig)

                g2 = sns.clustermap(dist_scaled, cmap="Blues_r", annot=True, fmt=".2f", figsize=(10, 7), metric="euclidean", method="ward")
                g2.fig.suptitle(f"Distance Clustering | Ref={ref_key} N={sample_size}")
                heat2_path = os.path.join(out_dir, f"{base_name}_distance_clustermap.png")
                g2.savefig(heat2_path, dpi=300, bbox_inches="tight")
                plt.close(g2.fig)
            except OSError as e:
                print(f"âš ï¸ ä¿å­˜çƒ­åŠ›å›¾å¤±è´¥ï¼š{e}")

            # å°ç»“
            if 'excel_path' in locals() and os.path.exists(excel_path):
                print(f"ğŸ“ æ’åºç»“æœï¼š{excel_path}")
            else:
                print(f"ğŸ“ æ’åºç»“æœï¼š{out_dir} ä¸‹ {base_name}_<method>.csv")
            if os.path.exists(fig_path):
                print(f"ğŸ–¼ï¸ å¯¹æ¯”å›¾ï¼š{fig_path}")


if __name__ == "__main__":
    main()



