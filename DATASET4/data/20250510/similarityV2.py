import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import wasserstein_distance, pearsonr, entropy
from sklearn.metrics.pairwise import cosine_similarity
from scipy.ndimage import gaussian_filter1d
import os
from matplotlib.cm import get_cmap

# -------- å‚æ•° --------
FOLDERS = ['20250510', '20250511', '20250512 6PM', '20250512 9AM', '20250513 5PM', '20250513 9AM',
           '20250514 9AM', '20250514 9PM', '20250515 9AM', '20250516 9AM', '20250517 9AM',
           '20250518 9AM', '20250519 9AM', '20250520 9AM', '20250521 9AM']
REFERENCE_FOLDER = '20250514 9AM'
AREA_COL = "area"
RATIO_OTHER = 0.5
RATIO_REF = 0.3
BINS = 60
RANGE = (500, 3500)
TOP_K = 5

# -------- ç›¸ä¼¼åº¦å‡½æ•° --------
def compute_histogram_similarity(hist1, hist2, method='cosine'):
    hist1 = np.asarray(hist1, dtype=np.float64) + 1e-10
    hist2 = np.asarray(hist2, dtype=np.float64) + 1e-10
    if method == 'cosine':
        return float(cosine_similarity([hist1], [hist2])[0][0])
    elif method == 'correlation':
        return pearsonr(hist1, hist2)[0]
    elif method == 'intersection':
        return np.sum(np.minimum(hist1, hist2)) / np.sum(np.maximum(hist1, hist2))
    elif method == 'chi2':
        return 0.5 * np.sum((hist1 - hist2) ** 2 / (hist1 + hist2))
    elif method == 'kl':
        return entropy(hist1, hist2)
    elif method == 'wasserstein':
        return wasserstein_distance(hist1, hist2)
    else:
        raise ValueError("Unsupported similarity method")

# -------- åŠ è½½ä¸Žé‡‡æ · --------
def load_histogram(folder, ratio):
    csv_path = os.path.join(folder, "total", "merged.csv")
    if not os.path.exists(csv_path):
        print(f"Warning: {csv_path} not found, skipping.")
        return None, None
    df = pd.read_csv(csv_path)
    
    # âœ… ä»…ä¿ç•™é¢ç§¯åœ¨æŒ‡å®šèŒƒå›´å†…çš„æ•°æ®
    area_data = df[AREA_COL].dropna()
    area_data = area_data[(area_data >= RANGE[0]) & (area_data <= RANGE[1])].values

    # å¦‚æžœæ•°æ®ä¸è¶³åˆ™è·³è¿‡
    if len(area_data) < 5:
        print(f"Warning: Too few area values in range for {folder}, skipping.")
        return None, None

    np.random.shuffle(area_data)
    sampled = area_data[:int(len(area_data) * ratio)]

    bins = np.linspace(RANGE[0], RANGE[1], BINS + 1)
    hist, _ = np.histogram(sampled, bins=bins, density=True)
    bin_centers = 0.5 * (bins[:-1] + bins[1:])
    return hist, bin_centers


# -------- ä¸»æ‰§è¡Œé€»è¾‘ --------
np.random.seed(42)

# 1. åŠ è½½å‚è€ƒç›´æ–¹å›¾ï¼ˆ60%é‡‡æ ·ï¼‰
ref_hist, bin_centers = load_histogram(REFERENCE_FOLDER, RATIO_REF)
if ref_hist is None:
    raise RuntimeError("å‚è€ƒæ–‡ä»¶è¯»å–å¤±è´¥ã€‚")

# 2. éåŽ†æ‰€æœ‰æ–‡ä»¶å¤¹ï¼Œæž„å»ºç›´æ–¹å›¾
histograms = {}
for folder in FOLDERS:
    hist, _ = load_histogram(folder, RATIO_OTHER)
    if hist is not None:
        histograms[folder] = hist

# 2.5 ç»Ÿè®¡æ¯ä¸ªæ–‡ä»¶å¤¹ä¸­ç»†èƒžæ€»æ•° & å¤„äºŽæŒ‡å®šèŒƒå›´å†…çš„ç»†èƒžæ•°
count_stats = []

for folder in FOLDERS:
    csv_path = os.path.join(folder, "total", "merged.csv")
    if not os.path.exists(csv_path):
        count_stats.append({
            "Folder": folder,
            "Total Cells": "File Not Found",
            "Cells in 500-3500": "N/A"
        })
        continue

    df = pd.read_csv(csv_path)
    area_series = df[AREA_COL].dropna()
    total_cells = len(area_series)
    in_range_cells = ((area_series >= RANGE[0]) & (area_series <= RANGE[1])).sum()

    count_stats.append({
        "Folder": folder,
        "Total Cells": total_cells,
        "Cells in 500-3500": in_range_cells
    })

# è½¬ä¸º DataFrame å¹¶å¯¼å‡º
count_df = pd.DataFrame(count_stats)
count_df.to_excel("results/cell_count_summary.xlsx", index=False)
print("\nðŸ“Š å·²ä¿å­˜ç»†èƒžæ•°é‡ç»Ÿè®¡è‡³ results/cell_count_summary.xlsx")


# 3. ç»˜åˆ¶æ‰€æœ‰å¹³æ»‘åŽçš„ç›´æ–¹å›¾
# plt.figure(figsize=(12, 6))
# plt.plot(bin_centers, gaussian_filter1d(ref_hist, sigma=2), label=f'{REFERENCE_FOLDER} (60%)', linewidth=2, color='black')
# for folder, hist in histograms.items():
#     smoothed_hist = gaussian_filter1d(hist, sigma=2)
#     plt.plot(bin_centers, smoothed_hist, label=f'{folder} (50%)', alpha=0.6)
# plt.title("Smoothed Area Distribution Histogram Curves")
# plt.xlabel("Cell Area")
# plt.ylabel("Normalized Frequency")
# plt.legend(fontsize=8)
# plt.tight_layout()
# plt.savefig("results/all_histograms_smoothed.png")
# plt.show()

# 3ï¸âƒ£ ç»˜åˆ¶æ‰€æœ‰å¹³æ»‘åŽçš„ç›´æ–¹å›¾ï¼Œå¢žå¼ºå¯¹æ¯”åº¦
plt.figure(figsize=(14, 7))
cmap = get_cmap("tab20", len(histograms))  # ä½¿ç”¨é«˜å¯¹æ¯”çš„tab20è°ƒè‰²æ¿
x = bin_centers

# ç»˜åˆ¶å‚è€ƒæ ·æœ¬æ›²çº¿
ref_smooth = gaussian_filter1d(ref_hist, sigma=2)
plt.plot(x, ref_smooth, label=f'{REFERENCE_FOLDER} (ref)', color='black', linewidth=2, zorder=10)

# ç»˜åˆ¶å…¶ä»–æ ·æœ¬æ›²çº¿
for idx, (folder, hist) in enumerate(histograms.items()):
    smooth = gaussian_filter1d(hist, sigma=2)
    color = cmap(idx)  # ä¸ºæ¯æ¡æ›²çº¿åˆ†é…ä¸åŒé¢œè‰²
    plt.plot(x, smooth, label=folder, color=color, alpha=0.85)

plt.title("Smoothed Area Distribution Histogram Curves")
plt.xlabel("Cell Area")
plt.ylabel("Normalized Frequency")
plt.legend(fontsize=8)
plt.tight_layout()
plt.savefig("results/all_histograms_smoothed.png")
plt.show()


# 4. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
similarity_data = []
for folder, hist in histograms.items():
    similarity_data.append({
        "Compared Folder": folder,
        "Cosine Similarity": compute_histogram_similarity(ref_hist, hist, 'cosine'),
        "Pearson Correlation": compute_histogram_similarity(ref_hist, hist, 'correlation'),
        "Histogram Intersection": compute_histogram_similarity(ref_hist, hist, 'intersection'),
        "Chi-Square Distance": compute_histogram_similarity(ref_hist, hist, 'chi2'),
        "KL Divergence": compute_histogram_similarity(ref_hist, hist, 'kl'),
        "Wasserstein Distance": compute_histogram_similarity(ref_hist, hist, 'wasserstein')
    })

similarity_df = pd.DataFrame(similarity_data)
similarity_df_sorted = similarity_df.sort_values(by="Histogram Intersection", ascending=False)

# 5. å¯¼å‡º Excel è¡¨æ ¼
excel_path = "results/histogram_similarity_results.xlsx"
similarity_df_sorted.to_excel(excel_path, index=False)

# 6. è¾“å‡ºæœ€ç›¸ä¼¼çš„å‰ TOP_K
top_k_similar = similarity_df_sorted.head(TOP_K)


# 7. å±‚æ¬¡èšç±»çƒ­åŠ›å›¾ï¼šå½’ä¸€åŒ– + æ›´åˆç†çš„é¢œè‰²æ˜ å°„
from sklearn.preprocessing import MinMaxScaler

# æ‹·è´æ•°æ®
similarity_data_copy = similarity_df.copy()
folders = similarity_data_copy["Compared Folder"]

# åˆ†ç»„
similarity_metrics = ["Cosine Similarity", "Pearson Correlation", "Histogram Intersection"]
distance_metrics = ["Chi-Square Distance", "KL Divergence", "Wasserstein Distance"]

# åˆ†åˆ«æå–ä¸¤ç»„æŒ‡æ ‡
similarity_part = similarity_data_copy[similarity_metrics]
distance_part = similarity_data_copy[distance_metrics]

# æ ‡å‡†åŒ–å¤„ç†ï¼ˆ0-1ï¼‰ä½¿é¢œè‰²å¼ºåº¦å¯æ¯”
sim_scaler = MinMaxScaler()
dist_scaler = MinMaxScaler()
similarity_scaled = pd.DataFrame(sim_scaler.fit_transform(similarity_part), 
                                 columns=similarity_part.columns,
                                 index=folders)

distance_scaled = pd.DataFrame(dist_scaler.fit_transform(distance_part), 
                               columns=distance_part.columns,
                               index=folders)

# 1ï¸âƒ£ ç›¸ä¼¼åº¦æŒ‡æ ‡èšç±»çƒ­åŠ›å›¾
sns.clustermap(similarity_scaled,
               cmap="Reds",  # çº¢è‰²è¡¨ç¤ºç›¸ä¼¼åº¦é«˜
               annot=True,
               fmt=".2f",
               figsize=(10, 7),
               metric="euclidean",
               method="ward")

plt.suptitle(f"Similarity Clustering to {REFERENCE_FOLDER} (High=Better)", fontsize=13)
plt.savefig("results/clustering_similarity_metrics.png")
plt.show()

# 2ï¸âƒ£ å·®å¼‚åº¦æŒ‡æ ‡èšç±»çƒ­åŠ›å›¾
sns.clustermap(distance_scaled,
               cmap="Blues_r",  # è“è‰²è¡¨ç¤ºå·®å¼‚åº¦å°æ›´å¥½ï¼ˆåè‰²ï¼‰
               annot=True,
               fmt=".2f",
               figsize=(10, 7),
               metric="euclidean",
               method="ward")

plt.suptitle(f"Distance Clustering to {REFERENCE_FOLDER} (Low=Better)", fontsize=13)
plt.savefig("results/clustering_distance_metrics.png")
plt.show()



# è¾“å‡ºæ‰€æœ‰å¯¹æ¯”ç»“æžœï¼ˆæŽ§åˆ¶å°ï¼‰
print(f"\nðŸ“Š Similarity of {REFERENCE_FOLDER+'æµ‹è¯•'} vs others:\n")
for folder, hist in histograms.items():
    cosine = compute_histogram_similarity(ref_hist, hist, 'cosine')
    corr = compute_histogram_similarity(ref_hist, hist, 'correlation')
    intersect = compute_histogram_similarity(ref_hist, hist, 'intersection')
    chi2 = compute_histogram_similarity(ref_hist, hist, 'chi2')
    kl = compute_histogram_similarity(ref_hist, hist, 'kl')
    wass = compute_histogram_similarity(ref_hist, hist, 'wasserstein')

    print(f"ðŸ”¹ {REFERENCE_FOLDER+'æµ‹è¯•'} vs {folder}")
    print(f"   Cosine Similarity       : {cosine:.4f}")
    print(f"   Pearson Correlation     : {corr:.4f}")
    print(f"   Histogram Intersection  : {intersect:.4f}")
    print(f"   Chi-Square Distance     : {chi2:.4f}")
    print(f"   KL Divergence           : {kl:.4f}")
    print(f"   Wasserstein Distance    : {wass:.4f}\n")

# è¾“å‡ºä¸Žå‚è€ƒæ ·æœ¬æœ€ç›¸ä¼¼çš„æ–‡ä»¶å¤¹ï¼ˆåŸºäºŽ Histogram Intersectionï¼‰
best_match = similarity_df_sorted.iloc[0]
print("ðŸ“Œ ä¸Žå‚è€ƒæ ·æœ¬æœ€ç›¸ä¼¼çš„æ›²çº¿ï¼ˆæŒ‰ Histogram Intersection æŽ’åºï¼‰:")
print(f"ðŸ”¸ æ–‡ä»¶å¤¹: {best_match['Compared Folder']}")
print(f"   Histogram Intersection: {best_match['Histogram Intersection']:.4f}")


